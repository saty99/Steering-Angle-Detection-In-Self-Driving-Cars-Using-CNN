{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36324\n",
      "36324 9082\n"
     ]
    }
   ],
   "source": [
    "import scipy.misc\n",
    "import random\n",
    "\n",
    "train_batch_pointer=0\n",
    "\n",
    "validation_batch_pointer=0\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "images=[]\n",
    "\n",
    "angles=[]\n",
    "\n",
    "from scipy import pi\n",
    "\n",
    "txt=open(\"driving_dataset/driving_dataset/data.txt\")\n",
    "num_lines=sum(1 for line in open(\"driving_dataset/driving_dataset/data.txt\"))\n",
    "lines_slice=islice(txt,num_lines)\n",
    "\n",
    "for line in lines_slice:\n",
    "\timage,angle=line.strip().split()\n",
    "\timages.append(\"driving_dataset/driving_dataset/\"+image)\n",
    "\tangle=float(angle)*scipy.pi/180\n",
    "\tangles.append(angle)\n",
    "\n",
    "#time based split\n",
    "split_ratio=0.8\n",
    "split_up_to=int(num_lines*split_ratio)\n",
    "print(split_up_to)\n",
    "train_images=images[:split_up_to]\n",
    "train_angles=angles[:split_up_to]\n",
    "\n",
    "validation_images=images[split_up_to:]\n",
    "validation_angles=angles[split_up_to:]\n",
    "\n",
    "num_train_images=len(train_images)\n",
    "num_validation_images=len(validation_images)\n",
    "print(num_train_images,num_validation_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time took: 0:00:00.015474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saireddyavs\\Anaconda3\\envs\\gpuu\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\saireddyavs\\Anaconda3\\envs\\gpuu\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time took: 0:00:16.861991\n",
      "time took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saireddyavs\\Anaconda3\\envs\\gpuu\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "C:\\Users\\saireddyavs\\Anaconda3\\envs\\gpuu\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time took: 0:01:18.953288\n",
      "time took:: 0:01:24.975230\n"
     ]
    }
   ],
   "source": [
    "train_ang=[]\n",
    "import datetime  as dt\n",
    "start=dt.datetime.now()\n",
    "for i in range(0,36324):\n",
    "    train_ang.append([train_angles[i]])\n",
    "    \n",
    "print(\"time took:\",dt.datetime.now()-start)\n",
    "\n",
    "\n",
    "\n",
    "val_im=[]\n",
    "\n",
    "import datetime  as dt\n",
    "start=dt.datetime.now()\n",
    "for i in range(0,9082):\n",
    "    val_im.append(scipy.misc.imresize(scipy.misc.imread(validation_images[ i ])[-150:], [66, 200]) / 255.0)\n",
    "print(\"time took:\",dt.datetime.now()-start)\n",
    "\n",
    "\n",
    "\n",
    "val_ang=[]\n",
    "import datetime  as dt\n",
    "start=dt.datetime.now()\n",
    "for i in range(0,9082):\n",
    "    val_ang.append([validation_angles[i]])\n",
    "    \n",
    "print(\"time took:\",dt.datetime.now()-start)\n",
    "\n",
    "\n",
    "train_im=[]\n",
    "import datetime  as dt\n",
    "start=dt.datetime.now()\n",
    "for i in range(0,36324):\n",
    "    train_im.append(scipy.misc.imresize(scipy.misc.imread(train_images[ i ])[-150:], [66, 200]) / 255.0)\n",
    "    \n",
    "print(\"time took:\",dt.datetime.now()-start)\n",
    "\n",
    "\n",
    "start = dt.datetime.now()\n",
    "\n",
    "import numpy as np\n",
    "train_im=np.array(train_im)\n",
    "val_im=np.array(val_im)\n",
    "val_ang=np.array(val_ang)\n",
    "train_ang=np.array(train_ang)\n",
    "\n",
    "print(\"time took::\",dt.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\saireddyavs\\Anaconda3\\envs\\gpuu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\saireddyavs\\Anaconda3\\envs\\gpuu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 31, 98, 24)        1824      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 47, 36)        21636     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 22, 48)         43248     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 20, 64)         27712     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 18, 64)         36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                57650     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 191,599\n",
      "Trainable params: 191,599\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense,Flatten,Dropout,Conv2D\n",
    "from keras import backend as k\n",
    "from keras.initializers import TruncatedNormal,Constant\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(24,5,strides=(2,2),activation=\"relu\",input_shape=(66,200,3),padding=\"valid\"))\n",
    "model.add(Conv2D(36,5,strides=(2,2),activation=\"relu\",padding=\"valid\"))\n",
    "model.add(Conv2D(48,5,strides=(2,2),activation=\"relu\",padding=\"valid\"))\n",
    "model.add(Conv2D(64,3,strides=(1,1),activation=\"relu\",padding=\"valid\"))\n",
    "model.add(Conv2D(64,3,strides=(1,1),activation=\"relu\",padding=\"valid\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(50,activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(50,activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "import time\n",
    "import keras\n",
    "class delay(keras.callbacks.Callback):\n",
    "    def __int__(self,delay_time=600):\n",
    "        sel.delay_time = 600\n",
    "        \n",
    "    def on_epoch_begin(self,epoch,logs={}):\n",
    "        if(epoch+1)%50==0:\n",
    "            print(\"cooling down to decrease CPU tempereture\")\n",
    "            time.sleep(600)#600 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\saireddyavs\\Anaconda3\\envs\\gpuu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 36324 samples, validate on 9082 samples\n",
      "Epoch 1/1000\n",
      "36324/36324 [==============================] - 49s 1ms/step - loss: 0.2922 - val_loss: 0.2713\n",
      "Epoch 2/1000\n",
      "36324/36324 [==============================] - 32s 883us/step - loss: 0.2337 - val_loss: 0.1811\n",
      "Epoch 3/1000\n",
      "36324/36324 [==============================] - 31s 851us/step - loss: 0.1736 - val_loss: 0.1701\n",
      "Epoch 4/1000\n",
      "36324/36324 [==============================] - 27s 745us/step - loss: 0.1503 - val_loss: 0.2379\n",
      "Epoch 5/1000\n",
      "36324/36324 [==============================] - 29s 807us/step - loss: 0.1039 - val_loss: 0.2152\n",
      "Epoch 6/1000\n",
      "36324/36324 [==============================] - 30s 835us/step - loss: 0.0831 - val_loss: 0.2354\n",
      "Epoch 7/1000\n",
      "36324/36324 [==============================] - 29s 799us/step - loss: 0.0698 - val_loss: 0.2350\n",
      "Epoch 8/1000\n",
      "36324/36324 [==============================] - 26s 706us/step - loss: 0.0694 - val_loss: 0.1926\n",
      "Epoch 9/1000\n",
      "36324/36324 [==============================] - 24s 665us/step - loss: 0.0624 - val_loss: 0.2870\n",
      "Epoch 10/1000\n",
      "36324/36324 [==============================] - 26s 725us/step - loss: 0.0534 - val_loss: 0.1878\n",
      "Epoch 11/1000\n",
      "36324/36324 [==============================] - 25s 678us/step - loss: 0.0464 - val_loss: 0.1886\n",
      "Epoch 12/1000\n",
      "36324/36324 [==============================] - 28s 759us/step - loss: 0.0460 - val_loss: 0.2057\n",
      "Epoch 13/1000\n",
      "36324/36324 [==============================] - 29s 796us/step - loss: 0.0452 - val_loss: 0.2295\n",
      "Epoch 14/1000\n",
      "36324/36324 [==============================] - 31s 858us/step - loss: 0.0410 - val_loss: 0.2168\n",
      "Epoch 15/1000\n",
      "36324/36324 [==============================] - 29s 799us/step - loss: 0.0413 - val_loss: 0.2141\n",
      "Epoch 16/1000\n",
      "36324/36324 [==============================] - 31s 862us/step - loss: 0.0417 - val_loss: 0.2088\n",
      "Epoch 17/1000\n",
      "36324/36324 [==============================] - 28s 769us/step - loss: 0.0377 - val_loss: 0.2268\n",
      "Epoch 18/1000\n",
      "36324/36324 [==============================] - 28s 778us/step - loss: 0.0377 - val_loss: 0.2371\n",
      "Epoch 19/1000\n",
      "36324/36324 [==============================] - 30s 827us/step - loss: 0.0339 - val_loss: 0.2068\n",
      "Epoch 20/1000\n",
      "36324/36324 [==============================] - 28s 784us/step - loss: 0.0334 - val_loss: 0.2115\n",
      "Epoch 21/1000\n",
      "36324/36324 [==============================] - 27s 742us/step - loss: 0.0350 - val_loss: 0.1995\n",
      "Epoch 22/1000\n",
      "36324/36324 [==============================] - 27s 754us/step - loss: 0.0320 - val_loss: 0.2175\n",
      "Epoch 23/1000\n",
      "36324/36324 [==============================] - 27s 748us/step - loss: 0.0352 - val_loss: 0.2225\n",
      "Epoch 24/1000\n",
      "36324/36324 [==============================] - 29s 788us/step - loss: 0.0333 - val_loss: 0.2494\n",
      "Epoch 25/1000\n",
      "36324/36324 [==============================] - 27s 754us/step - loss: 0.0320 - val_loss: 0.2154\n",
      "Epoch 26/1000\n",
      "36324/36324 [==============================] - 31s 843us/step - loss: 0.0309 - val_loss: 0.2314\n",
      "Epoch 27/1000\n",
      "36324/36324 [==============================] - 30s 824us/step - loss: 0.0333 - val_loss: 0.2340\n",
      "Epoch 28/1000\n",
      "36324/36324 [==============================] - 27s 734us/step - loss: 0.0306 - val_loss: 0.2616\n",
      "Epoch 29/1000\n",
      "36324/36324 [==============================] - 26s 717us/step - loss: 0.0319 - val_loss: 0.2471\n",
      "Epoch 30/1000\n",
      "36324/36324 [==============================] - 26s 727us/step - loss: 0.0299 - val_loss: 0.2189\n",
      "Epoch 31/1000\n",
      "36324/36324 [==============================] - 25s 700us/step - loss: 0.0308 - val_loss: 0.2593\n",
      "Epoch 32/1000\n",
      "36324/36324 [==============================] - 25s 682us/step - loss: 0.0304 - val_loss: 0.2625\n",
      "Epoch 33/1000\n",
      "36324/36324 [==============================] - 25s 695us/step - loss: 0.0272 - val_loss: 0.2150\n",
      "Epoch 34/1000\n",
      "36324/36324 [==============================] - 25s 688us/step - loss: 0.0258 - val_loss: 0.2185\n",
      "Epoch 35/1000\n",
      "36324/36324 [==============================] - 27s 755us/step - loss: 0.0289 - val_loss: 0.2471\n",
      "Epoch 36/1000\n",
      "36324/36324 [==============================] - 28s 766us/step - loss: 0.0273 - val_loss: 0.2352\n",
      "Epoch 37/1000\n",
      "36324/36324 [==============================] - 25s 677us/step - loss: 0.0253 - val_loss: 0.2477\n",
      "Epoch 38/1000\n",
      "36324/36324 [==============================] - 26s 719us/step - loss: 0.0243 - val_loss: 0.2558\n",
      "Epoch 39/1000\n",
      "36324/36324 [==============================] - 25s 701us/step - loss: 0.0269 - val_loss: 0.2250\n",
      "Epoch 40/1000\n",
      "36324/36324 [==============================] - 25s 690us/step - loss: 0.0236 - val_loss: 0.2306\n",
      "Epoch 41/1000\n",
      "36324/36324 [==============================] - 25s 686us/step - loss: 0.0232 - val_loss: 0.2170\n",
      "Epoch 42/1000\n",
      "36324/36324 [==============================] - 25s 682us/step - loss: 0.0255 - val_loss: 0.2683\n",
      "Epoch 43/1000\n",
      "36324/36324 [==============================] - 26s 713us/step - loss: 0.0252 - val_loss: 0.2226\n",
      "Epoch 44/1000\n",
      "36324/36324 [==============================] - 26s 715us/step - loss: 0.0253 - val_loss: 0.2361\n",
      "Epoch 45/1000\n",
      "36324/36324 [==============================] - 29s 797us/step - loss: 0.0229 - val_loss: 0.1713\n",
      "Epoch 46/1000\n",
      "36324/36324 [==============================] - 28s 775us/step - loss: 0.0244 - val_loss: 0.2764\n",
      "Epoch 47/1000\n",
      "36324/36324 [==============================] - 28s 770us/step - loss: 0.0254 - val_loss: 0.2149\n",
      "Epoch 48/1000\n",
      "36324/36324 [==============================] - 28s 777us/step - loss: 0.0219 - val_loss: 0.2548\n",
      "Epoch 49/1000\n",
      "36324/36324 [==============================] - 30s 827us/step - loss: 0.0238 - val_loss: 0.2510\n",
      "Epoch 50/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 632s 17ms/step - loss: 0.0218 - val_loss: 0.2295\n",
      "Epoch 51/1000\n",
      "36324/36324 [==============================] - 28s 774us/step - loss: 0.0228 - val_loss: 0.2520\n",
      "Epoch 52/1000\n",
      "36324/36324 [==============================] - 29s 790us/step - loss: 0.0247 - val_loss: 0.2008\n",
      "Epoch 53/1000\n",
      "36324/36324 [==============================] - 28s 761us/step - loss: 0.0262 - val_loss: 0.2304\n",
      "Epoch 54/1000\n",
      "36324/36324 [==============================] - 28s 769us/step - loss: 0.0214 - val_loss: 0.2138\n",
      "Epoch 55/1000\n",
      "36324/36324 [==============================] - 25s 693us/step - loss: 0.0235 - val_loss: 0.2307\n",
      "Epoch 56/1000\n",
      "36324/36324 [==============================] - 24s 659us/step - loss: 0.0215 - val_loss: 0.2406\n",
      "Epoch 57/1000\n",
      "36324/36324 [==============================] - 24s 651us/step - loss: 0.0246 - val_loss: 0.2652\n",
      "Epoch 58/1000\n",
      "36324/36324 [==============================] - 26s 704us/step - loss: 0.0214 - val_loss: 0.2599\n",
      "Epoch 59/1000\n",
      "36324/36324 [==============================] - 25s 677us/step - loss: 0.0223 - val_loss: 0.2562\n",
      "Epoch 60/1000\n",
      "36324/36324 [==============================] - 23s 640us/step - loss: 0.0236 - val_loss: 0.2822\n",
      "Epoch 61/1000\n",
      "36324/36324 [==============================] - 22s 614us/step - loss: 0.0209 - val_loss: 0.2051\n",
      "Epoch 62/1000\n",
      "36324/36324 [==============================] - 23s 632us/step - loss: 0.0211 - val_loss: 0.2055\n",
      "Epoch 63/1000\n",
      "36324/36324 [==============================] - 25s 693us/step - loss: 0.0205 - val_loss: 0.2341\n",
      "Epoch 64/1000\n",
      "36324/36324 [==============================] - 24s 659us/step - loss: 0.0240 - val_loss: 0.3317\n",
      "Epoch 65/1000\n",
      "36324/36324 [==============================] - 24s 671us/step - loss: 0.0201 - val_loss: 0.2396\n",
      "Epoch 66/1000\n",
      "36324/36324 [==============================] - 25s 683us/step - loss: 0.0227 - val_loss: 0.2344\n",
      "Epoch 67/1000\n",
      "36324/36324 [==============================] - 24s 648us/step - loss: 0.0224 - val_loss: 0.2569\n",
      "Epoch 68/1000\n",
      "36324/36324 [==============================] - 24s 660us/step - loss: 0.0211 - val_loss: 0.2709\n",
      "Epoch 69/1000\n",
      "36324/36324 [==============================] - 26s 715us/step - loss: 0.0215 - val_loss: 0.3005\n",
      "Epoch 70/1000\n",
      "36324/36324 [==============================] - 24s 669us/step - loss: 0.0204 - val_loss: 0.2753\n",
      "Epoch 71/1000\n",
      "36324/36324 [==============================] - 23s 638us/step - loss: 0.0209 - val_loss: 0.2321\n",
      "Epoch 72/1000\n",
      "36324/36324 [==============================] - 23s 636us/step - loss: 0.0218 - val_loss: 0.2136\n",
      "Epoch 73/1000\n",
      "36324/36324 [==============================] - 24s 668us/step - loss: 0.0206 - val_loss: 0.2286\n",
      "Epoch 74/1000\n",
      "36324/36324 [==============================] - 27s 745us/step - loss: 0.0206 - val_loss: 0.2508\n",
      "Epoch 75/1000\n",
      "36324/36324 [==============================] - 23s 631us/step - loss: 0.0212 - val_loss: 0.2617\n",
      "Epoch 76/1000\n",
      "36324/36324 [==============================] - 22s 613us/step - loss: 0.0206 - val_loss: 0.2855\n",
      "Epoch 77/1000\n",
      "36324/36324 [==============================] - 24s 653us/step - loss: 0.0209 - val_loss: 0.2870\n",
      "Epoch 78/1000\n",
      "36324/36324 [==============================] - 23s 643us/step - loss: 0.0197 - val_loss: 0.2226\n",
      "Epoch 79/1000\n",
      "36324/36324 [==============================] - 22s 616us/step - loss: 0.0209 - val_loss: 0.2440\n",
      "Epoch 80/1000\n",
      "36324/36324 [==============================] - 23s 637us/step - loss: 0.0219 - val_loss: 0.2443\n",
      "Epoch 81/1000\n",
      "36324/36324 [==============================] - 28s 759us/step - loss: 0.0190 - val_loss: 0.2299\n",
      "Epoch 82/1000\n",
      "36324/36324 [==============================] - 26s 722us/step - loss: 0.0185 - val_loss: 0.2521\n",
      "Epoch 83/1000\n",
      "36324/36324 [==============================] - 25s 698us/step - loss: 0.0182 - val_loss: 0.2861\n",
      "Epoch 84/1000\n",
      "36324/36324 [==============================] - 27s 738us/step - loss: 0.0187 - val_loss: 0.2138\n",
      "Epoch 85/1000\n",
      "36324/36324 [==============================] - 25s 701us/step - loss: 0.0177 - val_loss: 0.2641\n",
      "Epoch 86/1000\n",
      "36324/36324 [==============================] - 26s 723us/step - loss: 0.0179 - val_loss: 0.2622\n",
      "Epoch 87/1000\n",
      "36324/36324 [==============================] - 26s 708us/step - loss: 0.0183 - val_loss: 0.2379\n",
      "Epoch 88/1000\n",
      "36324/36324 [==============================] - 24s 670us/step - loss: 0.0174 - val_loss: 0.2311\n",
      "Epoch 89/1000\n",
      "36324/36324 [==============================] - 23s 632us/step - loss: 0.0180 - val_loss: 0.2619\n",
      "Epoch 90/1000\n",
      "36324/36324 [==============================] - 22s 608us/step - loss: 0.0179 - val_loss: 0.2752\n",
      "Epoch 91/1000\n",
      "36324/36324 [==============================] - 24s 655us/step - loss: 0.0172 - val_loss: 0.2694\n",
      "Epoch 92/1000\n",
      "36324/36324 [==============================] - 28s 771us/step - loss: 0.0169 - val_loss: 0.2490\n",
      "Epoch 93/1000\n",
      "36324/36324 [==============================] - 24s 669us/step - loss: 0.0187 - val_loss: 0.2523\n",
      "Epoch 94/1000\n",
      "36324/36324 [==============================] - 23s 642us/step - loss: 0.0194 - val_loss: 0.2480\n",
      "Epoch 95/1000\n",
      "36324/36324 [==============================] - 22s 603us/step - loss: 0.0201 - val_loss: 0.2283\n",
      "Epoch 96/1000\n",
      "36324/36324 [==============================] - 23s 620us/step - loss: 0.0216 - val_loss: 0.2391\n",
      "Epoch 97/1000\n",
      "36324/36324 [==============================] - 22s 619us/step - loss: 0.0204 - val_loss: 0.2869\n",
      "Epoch 98/1000\n",
      "36324/36324 [==============================] - 24s 657us/step - loss: 0.0215 - val_loss: 0.2499\n",
      "Epoch 99/1000\n",
      "36324/36324 [==============================] - 28s 770us/step - loss: 0.0201 - val_loss: 0.3843\n",
      "Epoch 100/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 634s 17ms/step - loss: 0.0193 - val_loss: 0.2688\n",
      "Epoch 101/1000\n",
      "36324/36324 [==============================] - 32s 877us/step - loss: 0.0188 - val_loss: 0.2773\n",
      "Epoch 102/1000\n",
      "36324/36324 [==============================] - 28s 765us/step - loss: 0.0179 - val_loss: 0.2875\n",
      "Epoch 103/1000\n",
      "36324/36324 [==============================] - 25s 700us/step - loss: 0.0167 - val_loss: 0.2912\n",
      "Epoch 104/1000\n",
      "36324/36324 [==============================] - 23s 643us/step - loss: 0.0172 - val_loss: 0.2690\n",
      "Epoch 105/1000\n",
      "36324/36324 [==============================] - 24s 662us/step - loss: 0.0166 - val_loss: 0.2234\n",
      "Epoch 106/1000\n",
      "36324/36324 [==============================] - 26s 706us/step - loss: 0.0182 - val_loss: 0.3201\n",
      "Epoch 107/1000\n",
      "36324/36324 [==============================] - 27s 743us/step - loss: 0.0171 - val_loss: 0.2668\n",
      "Epoch 108/1000\n",
      "36324/36324 [==============================] - 26s 720us/step - loss: 0.0173 - val_loss: 0.2966\n",
      "Epoch 109/1000\n",
      "36324/36324 [==============================] - 24s 661us/step - loss: 0.0171 - val_loss: 0.2890\n",
      "Epoch 110/1000\n",
      "36324/36324 [==============================] - 25s 685us/step - loss: 0.0179 - val_loss: 0.2557\n",
      "Epoch 111/1000\n",
      "36324/36324 [==============================] - 27s 742us/step - loss: 0.0183 - val_loss: 0.2590\n",
      "Epoch 112/1000\n",
      "36324/36324 [==============================] - 23s 633us/step - loss: 0.0177 - val_loss: 0.2538\n",
      "Epoch 113/1000\n",
      "36324/36324 [==============================] - 23s 639us/step - loss: 0.0193 - val_loss: 0.2733\n",
      "Epoch 114/1000\n",
      "36324/36324 [==============================] - 24s 655us/step - loss: 0.0181 - val_loss: 0.2783\n",
      "Epoch 115/1000\n",
      "36324/36324 [==============================] - 24s 660us/step - loss: 0.0168 - val_loss: 0.2562\n",
      "Epoch 116/1000\n",
      "36324/36324 [==============================] - 23s 637us/step - loss: 0.0173 - val_loss: 0.2653\n",
      "Epoch 117/1000\n",
      "36324/36324 [==============================] - 25s 676us/step - loss: 0.0166 - val_loss: 0.2594\n",
      "Epoch 118/1000\n",
      "36324/36324 [==============================] - 25s 674us/step - loss: 0.0152 - val_loss: 0.2909\n",
      "Epoch 119/1000\n",
      "36324/36324 [==============================] - 24s 674us/step - loss: 0.0180 - val_loss: 0.3184\n",
      "Epoch 120/1000\n",
      "36324/36324 [==============================] - 24s 653us/step - loss: 0.0156 - val_loss: 0.3037\n",
      "Epoch 121/1000\n",
      "36324/36324 [==============================] - 23s 641us/step - loss: 0.0157 - val_loss: 0.2487\n",
      "Epoch 122/1000\n",
      "36324/36324 [==============================] - 24s 652us/step - loss: 0.0164 - val_loss: 0.2292\n",
      "Epoch 123/1000\n",
      "36324/36324 [==============================] - 25s 680us/step - loss: 0.0164 - val_loss: 0.2545\n",
      "Epoch 124/1000\n",
      "36324/36324 [==============================] - 23s 645us/step - loss: 0.0158 - val_loss: 0.2550\n",
      "Epoch 125/1000\n",
      "36324/36324 [==============================] - 25s 679us/step - loss: 0.0157 - val_loss: 0.2650\n",
      "Epoch 126/1000\n",
      "36324/36324 [==============================] - 24s 658us/step - loss: 0.0176 - val_loss: 0.2783\n",
      "Epoch 127/1000\n",
      "36324/36324 [==============================] - 26s 704us/step - loss: 0.0169 - val_loss: 0.2688\n",
      "Epoch 128/1000\n",
      "36324/36324 [==============================] - 24s 651us/step - loss: 0.0179 - val_loss: 0.2406\n",
      "Epoch 129/1000\n",
      "36324/36324 [==============================] - 22s 617us/step - loss: 0.0175 - val_loss: 0.2933\n",
      "Epoch 130/1000\n",
      "36324/36324 [==============================] - 23s 622us/step - loss: 0.0161 - val_loss: 0.3160\n",
      "Epoch 131/1000\n",
      "36324/36324 [==============================] - 23s 641us/step - loss: 0.0183 - val_loss: 0.3580\n",
      "Epoch 132/1000\n",
      "36324/36324 [==============================] - 23s 638us/step - loss: 0.0168 - val_loss: 0.2311\n",
      "Epoch 133/1000\n",
      "36324/36324 [==============================] - 21s 576us/step - loss: 0.0158 - val_loss: 0.2926\n",
      "Epoch 134/1000\n",
      "36324/36324 [==============================] - 24s 648us/step - loss: 0.0161 - val_loss: 0.2756\n",
      "Epoch 135/1000\n",
      "36324/36324 [==============================] - 21s 575us/step - loss: 0.0162 - val_loss: 0.3094\n",
      "Epoch 136/1000\n",
      "36324/36324 [==============================] - 21s 583us/step - loss: 0.0159 - val_loss: 0.2805\n",
      "Epoch 137/1000\n",
      "36324/36324 [==============================] - 22s 618us/step - loss: 0.0179 - val_loss: 0.2604\n",
      "Epoch 138/1000\n",
      "36324/36324 [==============================] - 25s 692us/step - loss: 0.0173 - val_loss: 0.2605\n",
      "Epoch 139/1000\n",
      "36324/36324 [==============================] - 24s 658us/step - loss: 0.0162 - val_loss: 0.2794\n",
      "Epoch 140/1000\n",
      "36324/36324 [==============================] - 23s 644us/step - loss: 0.0156 - val_loss: 0.3389\n",
      "Epoch 141/1000\n",
      "36324/36324 [==============================] - 22s 595us/step - loss: 0.0145 - val_loss: 0.3152\n",
      "Epoch 142/1000\n",
      "36324/36324 [==============================] - 24s 652us/step - loss: 0.0160 - val_loss: 0.3601\n",
      "Epoch 143/1000\n",
      "36324/36324 [==============================] - 34s 937us/step - loss: 0.0158 - val_loss: 0.2870\n",
      "Epoch 144/1000\n",
      "36324/36324 [==============================] - 24s 671us/step - loss: 0.0157 - val_loss: 0.3303\n",
      "Epoch 145/1000\n",
      "36324/36324 [==============================] - 23s 620us/step - loss: 0.0152 - val_loss: 0.2742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/1000\n",
      "36324/36324 [==============================] - 22s 601us/step - loss: 0.0156 - val_loss: 0.2936\n",
      "Epoch 147/1000\n",
      "36324/36324 [==============================] - 23s 640us/step - loss: 0.0176 - val_loss: 0.2879\n",
      "Epoch 148/1000\n",
      "36324/36324 [==============================] - 23s 645us/step - loss: 0.0152 - val_loss: 0.2967\n",
      "Epoch 149/1000\n",
      "36324/36324 [==============================] - 24s 654us/step - loss: 0.0146 - val_loss: 0.2765\n",
      "Epoch 150/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 631s 17ms/step - loss: 0.0150 - val_loss: 0.2627\n",
      "Epoch 151/1000\n",
      "36324/36324 [==============================] - 27s 742us/step - loss: 0.0154 - val_loss: 0.2755\n",
      "Epoch 152/1000\n",
      "36324/36324 [==============================] - 28s 784us/step - loss: 0.0152 - val_loss: 0.2930\n",
      "Epoch 153/1000\n",
      "36324/36324 [==============================] - 26s 711us/step - loss: 0.0154 - val_loss: 0.2677\n",
      "Epoch 154/1000\n",
      "36324/36324 [==============================] - 26s 712us/step - loss: 0.0163 - val_loss: 0.2730\n",
      "Epoch 155/1000\n",
      "36324/36324 [==============================] - 26s 728us/step - loss: 0.0153 - val_loss: 0.2735\n",
      "Epoch 156/1000\n",
      "36324/36324 [==============================] - 27s 735us/step - loss: 0.0162 - val_loss: 0.3160\n",
      "Epoch 157/1000\n",
      "36324/36324 [==============================] - 25s 678us/step - loss: 0.0161 - val_loss: 0.2779\n",
      "Epoch 158/1000\n",
      "36324/36324 [==============================] - 26s 722us/step - loss: 0.0159 - val_loss: 0.3169\n",
      "Epoch 159/1000\n",
      "36324/36324 [==============================] - 24s 667us/step - loss: 0.0159 - val_loss: 0.2808\n",
      "Epoch 160/1000\n",
      "36324/36324 [==============================] - 26s 704us/step - loss: 0.0152 - val_loss: 0.3022\n",
      "Epoch 161/1000\n",
      "36324/36324 [==============================] - 27s 743us/step - loss: 0.0155 - val_loss: 0.3035\n",
      "Epoch 162/1000\n",
      "36324/36324 [==============================] - 27s 735us/step - loss: 0.0148 - val_loss: 0.4068\n",
      "Epoch 163/1000\n",
      "36324/36324 [==============================] - 25s 695us/step - loss: 0.0156 - val_loss: 0.2745\n",
      "Epoch 164/1000\n",
      "36324/36324 [==============================] - 22s 615us/step - loss: 0.0140 - val_loss: 0.2899\n",
      "Epoch 165/1000\n",
      "36324/36324 [==============================] - 25s 699us/step - loss: 0.0156 - val_loss: 0.3614\n",
      "Epoch 166/1000\n",
      "36324/36324 [==============================] - 23s 636us/step - loss: 0.0156 - val_loss: 0.3705\n",
      "Epoch 167/1000\n",
      "36324/36324 [==============================] - 25s 684us/step - loss: 0.0161 - val_loss: 0.2855\n",
      "Epoch 168/1000\n",
      "36324/36324 [==============================] - 24s 671us/step - loss: 0.0164 - val_loss: 0.2945\n",
      "Epoch 169/1000\n",
      "36324/36324 [==============================] - 24s 669us/step - loss: 0.0184 - val_loss: 0.2621\n",
      "Epoch 170/1000\n",
      "36324/36324 [==============================] - 24s 657us/step - loss: 0.0146 - val_loss: 0.2531\n",
      "Epoch 171/1000\n",
      "36324/36324 [==============================] - 25s 675us/step - loss: 0.0141 - val_loss: 0.2761\n",
      "Epoch 172/1000\n",
      "36324/36324 [==============================] - 25s 695us/step - loss: 0.0138 - val_loss: 0.2766\n",
      "Epoch 173/1000\n",
      "36324/36324 [==============================] - 23s 639us/step - loss: 0.0151 - val_loss: 0.2630\n",
      "Epoch 174/1000\n",
      "36324/36324 [==============================] - 27s 732us/step - loss: 0.0147 - val_loss: 0.2972\n",
      "Epoch 175/1000\n",
      "36324/36324 [==============================] - 25s 687us/step - loss: 0.0140 - val_loss: 0.2933\n",
      "Epoch 176/1000\n",
      "36324/36324 [==============================] - 24s 664us/step - loss: 0.0142 - val_loss: 0.3186\n",
      "Epoch 177/1000\n",
      "36324/36324 [==============================] - 24s 652us/step - loss: 0.0141 - val_loss: 0.2589\n",
      "Epoch 178/1000\n",
      "36324/36324 [==============================] - 22s 611us/step - loss: 0.0163 - val_loss: 0.2697\n",
      "Epoch 179/1000\n",
      "36324/36324 [==============================] - 23s 636us/step - loss: 0.0158 - val_loss: 0.2538\n",
      "Epoch 180/1000\n",
      "36324/36324 [==============================] - 24s 672us/step - loss: 0.0145 - val_loss: 0.2249\n",
      "Epoch 181/1000\n",
      "36324/36324 [==============================] - 26s 704us/step - loss: 0.0147 - val_loss: 0.3117\n",
      "Epoch 182/1000\n",
      "36324/36324 [==============================] - 23s 642us/step - loss: 0.0135 - val_loss: 0.2404\n",
      "Epoch 183/1000\n",
      "36324/36324 [==============================] - 29s 801us/step - loss: 0.0149 - val_loss: 0.2693\n",
      "Epoch 184/1000\n",
      "36324/36324 [==============================] - 28s 781us/step - loss: 0.0152 - val_loss: 0.3306\n",
      "Epoch 185/1000\n",
      "36324/36324 [==============================] - 27s 735us/step - loss: 0.0141 - val_loss: 0.2917\n",
      "Epoch 186/1000\n",
      "36324/36324 [==============================] - 26s 725us/step - loss: 0.0148 - val_loss: 0.3874\n",
      "Epoch 187/1000\n",
      "36324/36324 [==============================] - 26s 719us/step - loss: 0.0144 - val_loss: 0.2954\n",
      "Epoch 188/1000\n",
      "36324/36324 [==============================] - 26s 722us/step - loss: 0.0165 - val_loss: 0.2723\n",
      "Epoch 189/1000\n",
      "36324/36324 [==============================] - 26s 723us/step - loss: 0.0152 - val_loss: 0.3350\n",
      "Epoch 190/1000\n",
      "36324/36324 [==============================] - 24s 656us/step - loss: 0.0150 - val_loss: 0.2518\n",
      "Epoch 191/1000\n",
      "36324/36324 [==============================] - 24s 664us/step - loss: 0.0140 - val_loss: 0.3167\n",
      "Epoch 192/1000\n",
      "36324/36324 [==============================] - 23s 637us/step - loss: 0.0141 - val_loss: 0.3354\n",
      "Epoch 193/1000\n",
      "36324/36324 [==============================] - 25s 682us/step - loss: 0.0150 - val_loss: 0.2278\n",
      "Epoch 194/1000\n",
      "36324/36324 [==============================] - 22s 604us/step - loss: 0.0139 - val_loss: 0.3567\n",
      "Epoch 195/1000\n",
      "36324/36324 [==============================] - 24s 664us/step - loss: 0.0146 - val_loss: 0.2599\n",
      "Epoch 196/1000\n",
      "36324/36324 [==============================] - 23s 640us/step - loss: 0.0145 - val_loss: 0.3634\n",
      "Epoch 197/1000\n",
      "36324/36324 [==============================] - 21s 590us/step - loss: 0.0148 - val_loss: 0.2996\n",
      "Epoch 198/1000\n",
      "36324/36324 [==============================] - 21s 584us/step - loss: 0.0145 - val_loss: 0.3294\n",
      "Epoch 199/1000\n",
      "36324/36324 [==============================] - 23s 621us/step - loss: 0.0132 - val_loss: 0.3179\n",
      "Epoch 200/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 637s 18ms/step - loss: 0.0150 - val_loss: 0.2951\n",
      "Epoch 201/1000\n",
      "36324/36324 [==============================] - 29s 792us/step - loss: 0.0153 - val_loss: 0.3118\n",
      "Epoch 202/1000\n",
      "36324/36324 [==============================] - 26s 726us/step - loss: 0.0154 - val_loss: 0.3043\n",
      "Epoch 203/1000\n",
      "36324/36324 [==============================] - 43s 1ms/step - loss: 0.0138 - val_loss: 0.3167\n",
      "Epoch 204/1000\n",
      "36324/36324 [==============================] - 53s 1ms/step - loss: 0.0148 - val_loss: 0.3199\n",
      "Epoch 205/1000\n",
      "36324/36324 [==============================] - 24s 671us/step - loss: 0.0196 - val_loss: 0.2361\n",
      "Epoch 206/1000\n",
      "36324/36324 [==============================] - 25s 690us/step - loss: 0.0155 - val_loss: 0.2632\n",
      "Epoch 207/1000\n",
      "36324/36324 [==============================] - 24s 669us/step - loss: 0.0153 - val_loss: 0.3305\n",
      "Epoch 208/1000\n",
      "36324/36324 [==============================] - 23s 636us/step - loss: 0.0148 - val_loss: 0.2982\n",
      "Epoch 209/1000\n",
      "36324/36324 [==============================] - 26s 711us/step - loss: 0.0134 - val_loss: 0.3145\n",
      "Epoch 210/1000\n",
      "36324/36324 [==============================] - 23s 646us/step - loss: 0.0152 - val_loss: 0.2918\n",
      "Epoch 211/1000\n",
      "36324/36324 [==============================] - 23s 633us/step - loss: 0.0146 - val_loss: 0.2995\n",
      "Epoch 212/1000\n",
      "36324/36324 [==============================] - 23s 646us/step - loss: 0.0141 - val_loss: 0.3097\n",
      "Epoch 213/1000\n",
      "36324/36324 [==============================] - 23s 633us/step - loss: 0.0134 - val_loss: 0.2749\n",
      "Epoch 214/1000\n",
      "36324/36324 [==============================] - 24s 650us/step - loss: 0.0132 - val_loss: 0.3159\n",
      "Epoch 215/1000\n",
      "36324/36324 [==============================] - 23s 621us/step - loss: 0.0137 - val_loss: 0.2640\n",
      "Epoch 216/1000\n",
      "36324/36324 [==============================] - 25s 677us/step - loss: 0.0138 - val_loss: 0.2579\n",
      "Epoch 217/1000\n",
      "36324/36324 [==============================] - 25s 699us/step - loss: 0.0142 - val_loss: 0.2931\n",
      "Epoch 218/1000\n",
      "36324/36324 [==============================] - 23s 625us/step - loss: 0.0127 - val_loss: 0.2830\n",
      "Epoch 219/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36324/36324 [==============================] - 22s 601us/step - loss: 0.0134 - val_loss: 0.2893\n",
      "Epoch 220/1000\n",
      "36324/36324 [==============================] - 23s 630us/step - loss: 0.0131 - val_loss: 0.2512\n",
      "Epoch 221/1000\n",
      "36324/36324 [==============================] - 21s 591us/step - loss: 0.0137 - val_loss: 0.2720\n",
      "Epoch 222/1000\n",
      "36324/36324 [==============================] - 23s 635us/step - loss: 0.0135 - val_loss: 0.3139\n",
      "Epoch 223/1000\n",
      "36324/36324 [==============================] - 21s 577us/step - loss: 0.0126 - val_loss: 0.2950\n",
      "Epoch 224/1000\n",
      "36324/36324 [==============================] - 21s 583us/step - loss: 0.0133 - val_loss: 0.2948\n",
      "Epoch 225/1000\n",
      "36324/36324 [==============================] - 21s 586us/step - loss: 0.0124 - val_loss: 0.2962\n",
      "Epoch 226/1000\n",
      "36324/36324 [==============================] - 23s 645us/step - loss: 0.0128 - val_loss: 0.2531\n",
      "Epoch 227/1000\n",
      "36324/36324 [==============================] - 24s 663us/step - loss: 0.0126 - val_loss: 0.2810\n",
      "Epoch 228/1000\n",
      "36324/36324 [==============================] - 22s 606us/step - loss: 0.0130 - val_loss: 0.3123\n",
      "Epoch 229/1000\n",
      "36324/36324 [==============================] - 22s 606us/step - loss: 0.0129 - val_loss: 0.2407\n",
      "Epoch 230/1000\n",
      "36324/36324 [==============================] - 23s 628us/step - loss: 0.0138 - val_loss: 0.2728\n",
      "Epoch 231/1000\n",
      "36324/36324 [==============================] - 55s 2ms/step - loss: 0.0142 - val_loss: 0.3429\n",
      "Epoch 232/1000\n",
      "36324/36324 [==============================] - 49s 1ms/step - loss: 0.0134 - val_loss: 0.2579\n",
      "Epoch 233/1000\n",
      "36324/36324 [==============================] - 27s 734us/step - loss: 0.0126 - val_loss: 0.2685\n",
      "Epoch 234/1000\n",
      "36324/36324 [==============================] - 27s 744us/step - loss: 0.0125 - val_loss: 0.3134\n",
      "Epoch 235/1000\n",
      "36324/36324 [==============================] - 27s 734us/step - loss: 0.0120 - val_loss: 0.2904\n",
      "Epoch 236/1000\n",
      "36324/36324 [==============================] - 39s 1ms/step - loss: 0.0138 - val_loss: 0.3090\n",
      "Epoch 237/1000\n",
      "36324/36324 [==============================] - 26s 729us/step - loss: 0.0137 - val_loss: 0.4358\n",
      "Epoch 238/1000\n",
      "36324/36324 [==============================] - 25s 696us/step - loss: 0.0124 - val_loss: 0.2395\n",
      "Epoch 239/1000\n",
      "36324/36324 [==============================] - 29s 796us/step - loss: 0.0131 - val_loss: 0.3451\n",
      "Epoch 240/1000\n",
      "36324/36324 [==============================] - 34s 938us/step - loss: 0.0144 - val_loss: 0.2549\n",
      "Epoch 241/1000\n",
      "36324/36324 [==============================] - 24s 673us/step - loss: 0.0141 - val_loss: 0.2828\n",
      "Epoch 242/1000\n",
      "36324/36324 [==============================] - 25s 681us/step - loss: 0.0143 - val_loss: 0.3381\n",
      "Epoch 243/1000\n",
      "36324/36324 [==============================] - 24s 668us/step - loss: 0.0140 - val_loss: 0.3232\n",
      "Epoch 244/1000\n",
      "36324/36324 [==============================] - 25s 702us/step - loss: 0.0133 - val_loss: 0.2954\n",
      "Epoch 245/1000\n",
      "36324/36324 [==============================] - 25s 693us/step - loss: 0.0126 - val_loss: 0.3430\n",
      "Epoch 246/1000\n",
      "36324/36324 [==============================] - 24s 662us/step - loss: 0.0131 - val_loss: 0.2672\n",
      "Epoch 247/1000\n",
      "36324/36324 [==============================] - 27s 747us/step - loss: 0.0138 - val_loss: 0.3130\n",
      "Epoch 248/1000\n",
      "36324/36324 [==============================] - 29s 808us/step - loss: 0.0150 - val_loss: 0.3270\n",
      "Epoch 249/1000\n",
      "36324/36324 [==============================] - 27s 753us/step - loss: 0.0129 - val_loss: 0.2768\n",
      "Epoch 250/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 626s 17ms/step - loss: 0.0131 - val_loss: 0.4014\n",
      "Epoch 251/1000\n",
      "36324/36324 [==============================] - 26s 712us/step - loss: 0.0153 - val_loss: 0.3364\n",
      "Epoch 252/1000\n",
      "36324/36324 [==============================] - 24s 654us/step - loss: 0.0136 - val_loss: 0.3146\n",
      "Epoch 253/1000\n",
      "36324/36324 [==============================] - 23s 635us/step - loss: 0.0134 - val_loss: 0.3049\n",
      "Epoch 254/1000\n",
      "36324/36324 [==============================] - 29s 792us/step - loss: 0.0140 - val_loss: 0.2450\n",
      "Epoch 255/1000\n",
      "36324/36324 [==============================] - 27s 742us/step - loss: 0.0135 - val_loss: 0.3471\n",
      "Epoch 256/1000\n",
      "36324/36324 [==============================] - 25s 677us/step - loss: 0.0130 - val_loss: 0.2968\n",
      "Epoch 257/1000\n",
      "36324/36324 [==============================] - 23s 644us/step - loss: 0.0137 - val_loss: 0.2854\n",
      "Epoch 258/1000\n",
      "36324/36324 [==============================] - 25s 698us/step - loss: 0.0132 - val_loss: 0.3153\n",
      "Epoch 259/1000\n",
      "36324/36324 [==============================] - 24s 666us/step - loss: 0.0125 - val_loss: 0.2744\n",
      "Epoch 260/1000\n",
      "36324/36324 [==============================] - 27s 730us/step - loss: 0.0149 - val_loss: 0.2753\n",
      "Epoch 261/1000\n",
      "36324/36324 [==============================] - 26s 726us/step - loss: 0.0146 - val_loss: 0.2967\n",
      "Epoch 262/1000\n",
      "36324/36324 [==============================] - 24s 673us/step - loss: 0.0148 - val_loss: 0.3176\n",
      "Epoch 263/1000\n",
      "36324/36324 [==============================] - 27s 735us/step - loss: 0.0154 - val_loss: 0.3180\n",
      "Epoch 264/1000\n",
      "36324/36324 [==============================] - 22s 601us/step - loss: 0.0126 - val_loss: 0.3099\n",
      "Epoch 265/1000\n",
      "36324/36324 [==============================] - 22s 595us/step - loss: 0.0131 - val_loss: 0.3588\n",
      "Epoch 266/1000\n",
      "36324/36324 [==============================] - 22s 606us/step - loss: 0.0133 - val_loss: 0.3163\n",
      "Epoch 267/1000\n",
      "36324/36324 [==============================] - 23s 633us/step - loss: 0.0126 - val_loss: 0.2822\n",
      "Epoch 268/1000\n",
      "36324/36324 [==============================] - 23s 643us/step - loss: 0.0135 - val_loss: 0.3229\n",
      "Epoch 269/1000\n",
      "36324/36324 [==============================] - 31s 858us/step - loss: 0.0138 - val_loss: 0.2988\n",
      "Epoch 270/1000\n",
      "36324/36324 [==============================] - 24s 655us/step - loss: 0.0125 - val_loss: 0.2668\n",
      "Epoch 271/1000\n",
      "36324/36324 [==============================] - 22s 596us/step - loss: 0.0130 - val_loss: 0.2980\n",
      "Epoch 272/1000\n",
      "36324/36324 [==============================] - 24s 666us/step - loss: 0.0153 - val_loss: 0.2672\n",
      "Epoch 273/1000\n",
      "36324/36324 [==============================] - 35s 970us/step - loss: 0.0140 - val_loss: 0.3001\n",
      "Epoch 274/1000\n",
      "36324/36324 [==============================] - 31s 853us/step - loss: 0.0131 - val_loss: 0.2796\n",
      "Epoch 275/1000\n",
      "36324/36324 [==============================] - 75s 2ms/step - loss: 0.0147 - val_loss: 0.2669\n",
      "Epoch 276/1000\n",
      "36324/36324 [==============================] - 31s 859us/step - loss: 0.0163 - val_loss: 0.3142\n",
      "Epoch 277/1000\n",
      "36324/36324 [==============================] - 26s 728us/step - loss: 0.0151 - val_loss: 0.3267\n",
      "Epoch 278/1000\n",
      "36324/36324 [==============================] - 28s 767us/step - loss: 0.0128 - val_loss: 0.3406\n",
      "Epoch 279/1000\n",
      "36324/36324 [==============================] - 27s 750us/step - loss: 0.0130 - val_loss: 0.3177\n",
      "Epoch 280/1000\n",
      "36324/36324 [==============================] - 24s 672us/step - loss: 0.0133 - val_loss: 0.3378\n",
      "Epoch 281/1000\n",
      "36324/36324 [==============================] - 23s 645us/step - loss: 0.0139 - val_loss: 0.3232\n",
      "Epoch 282/1000\n",
      "36324/36324 [==============================] - 24s 673us/step - loss: 0.0135 - val_loss: 0.2639\n",
      "Epoch 283/1000\n",
      "36324/36324 [==============================] - 25s 696us/step - loss: 0.0135 - val_loss: 0.2753\n",
      "Epoch 284/1000\n",
      "36324/36324 [==============================] - 24s 671us/step - loss: 0.0120 - val_loss: 0.2796\n",
      "Epoch 285/1000\n",
      "36324/36324 [==============================] - 27s 743us/step - loss: 0.0127 - val_loss: 0.2978\n",
      "Epoch 286/1000\n",
      "36324/36324 [==============================] - 24s 650us/step - loss: 0.0140 - val_loss: 0.3108\n",
      "Epoch 287/1000\n",
      "36324/36324 [==============================] - 23s 631us/step - loss: 0.0140 - val_loss: 0.3162\n",
      "Epoch 288/1000\n",
      "36324/36324 [==============================] - 23s 643us/step - loss: 0.0132 - val_loss: 0.3484\n",
      "Epoch 289/1000\n",
      "36324/36324 [==============================] - 24s 662us/step - loss: 0.0163 - val_loss: 0.3241\n",
      "Epoch 290/1000\n",
      "36324/36324 [==============================] - 25s 681us/step - loss: 0.0140 - val_loss: 0.2409\n",
      "Epoch 291/1000\n",
      "36324/36324 [==============================] - 22s 601us/step - loss: 0.0131 - val_loss: 0.3731\n",
      "Epoch 292/1000\n",
      "36324/36324 [==============================] - 24s 663us/step - loss: 0.0137 - val_loss: 0.3021\n",
      "Epoch 293/1000\n",
      "36324/36324 [==============================] - 28s 775us/step - loss: 0.0126 - val_loss: 0.2785\n",
      "Epoch 294/1000\n",
      "36324/36324 [==============================] - 27s 735us/step - loss: 0.0128 - val_loss: 0.2919\n",
      "Epoch 295/1000\n",
      "36324/36324 [==============================] - 25s 682us/step - loss: 0.0121 - val_loss: 0.2826\n",
      "Epoch 296/1000\n",
      "36324/36324 [==============================] - 27s 743us/step - loss: 0.0140 - val_loss: 0.3018\n",
      "Epoch 297/1000\n",
      "36324/36324 [==============================] - 24s 655us/step - loss: 0.0129 - val_loss: 0.3195\n",
      "Epoch 298/1000\n",
      "36324/36324 [==============================] - 27s 736us/step - loss: 0.0131 - val_loss: 0.2621\n",
      "Epoch 299/1000\n",
      "36324/36324 [==============================] - 25s 693us/step - loss: 0.0121 - val_loss: 0.2724\n",
      "Epoch 300/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 639s 18ms/step - loss: 0.0129 - val_loss: 0.3031\n",
      "Epoch 301/1000\n",
      "36324/36324 [==============================] - 29s 790us/step - loss: 0.0121 - val_loss: 0.3514\n",
      "Epoch 302/1000\n",
      "36324/36324 [==============================] - 27s 741us/step - loss: 0.0136 - val_loss: 0.2645\n",
      "Epoch 303/1000\n",
      "36324/36324 [==============================] - 25s 700us/step - loss: 0.0117 - val_loss: 0.3243\n",
      "Epoch 304/1000\n",
      "36324/36324 [==============================] - 25s 676us/step - loss: 0.0117 - val_loss: 0.3042\n",
      "Epoch 305/1000\n",
      "36324/36324 [==============================] - 24s 657us/step - loss: 0.0125 - val_loss: 0.3442\n",
      "Epoch 306/1000\n",
      "36324/36324 [==============================] - 25s 681us/step - loss: 0.0124 - val_loss: 0.3256\n",
      "Epoch 307/1000\n",
      "36324/36324 [==============================] - 25s 685us/step - loss: 0.0126 - val_loss: 0.2995\n",
      "Epoch 308/1000\n",
      "36324/36324 [==============================] - 27s 735us/step - loss: 0.0123 - val_loss: 0.3231\n",
      "Epoch 309/1000\n",
      "36324/36324 [==============================] - 24s 670us/step - loss: 0.0135 - val_loss: 0.3242\n",
      "Epoch 310/1000\n",
      "36324/36324 [==============================] - 23s 637us/step - loss: 0.0135 - val_loss: 0.3066\n",
      "Epoch 311/1000\n",
      "36324/36324 [==============================] - 22s 601us/step - loss: 0.0132 - val_loss: 0.2824\n",
      "Epoch 312/1000\n",
      "36324/36324 [==============================] - 26s 713us/step - loss: 0.0135 - val_loss: 0.3637\n",
      "Epoch 313/1000\n",
      "36324/36324 [==============================] - 24s 673us/step - loss: 0.0140 - val_loss: 0.3395\n",
      "Epoch 314/1000\n",
      "36324/36324 [==============================] - 24s 655us/step - loss: 0.0130 - val_loss: 0.3419\n",
      "Epoch 315/1000\n",
      "36324/36324 [==============================] - 24s 648us/step - loss: 0.0122 - val_loss: 0.3358\n",
      "Epoch 316/1000\n",
      "36324/36324 [==============================] - 24s 667us/step - loss: 0.0127 - val_loss: 0.2581\n",
      "Epoch 317/1000\n",
      "36324/36324 [==============================] - 26s 727us/step - loss: 0.0136 - val_loss: 0.3771\n",
      "Epoch 318/1000\n",
      "36324/36324 [==============================] - 28s 766us/step - loss: 0.0128 - val_loss: 0.2549\n",
      "Epoch 319/1000\n",
      "36324/36324 [==============================] - 27s 730us/step - loss: 0.0123 - val_loss: 0.3009\n",
      "Epoch 320/1000\n",
      "36324/36324 [==============================] - 28s 758us/step - loss: 0.0119 - val_loss: 0.3229\n",
      "Epoch 321/1000\n",
      "36324/36324 [==============================] - 25s 692us/step - loss: 0.0125 - val_loss: 0.2358\n",
      "Epoch 322/1000\n",
      "36324/36324 [==============================] - 25s 692us/step - loss: 0.0119 - val_loss: 0.3287\n",
      "Epoch 323/1000\n",
      "36324/36324 [==============================] - 24s 666us/step - loss: 0.0133 - val_loss: 0.3350\n",
      "Epoch 324/1000\n",
      "36324/36324 [==============================] - 26s 707us/step - loss: 0.0122 - val_loss: 0.3664\n",
      "Epoch 325/1000\n",
      "36324/36324 [==============================] - 27s 740us/step - loss: 0.0134 - val_loss: 0.2692\n",
      "Epoch 326/1000\n",
      "36324/36324 [==============================] - 28s 763us/step - loss: 0.0127 - val_loss: 0.2597\n",
      "Epoch 327/1000\n",
      "36324/36324 [==============================] - 26s 708us/step - loss: 0.0131 - val_loss: 0.3351\n",
      "Epoch 328/1000\n",
      "36324/36324 [==============================] - 25s 687us/step - loss: 0.0123 - val_loss: 0.2938\n",
      "Epoch 329/1000\n",
      "36324/36324 [==============================] - 24s 665us/step - loss: 0.0134 - val_loss: 0.2622\n",
      "Epoch 330/1000\n",
      "36324/36324 [==============================] - 24s 662us/step - loss: 0.0137 - val_loss: 0.3090\n",
      "Epoch 331/1000\n",
      "36324/36324 [==============================] - 24s 666us/step - loss: 0.0129 - val_loss: 0.3096\n",
      "Epoch 332/1000\n",
      "36324/36324 [==============================] - 23s 637us/step - loss: 0.0118 - val_loss: 0.3559\n",
      "Epoch 333/1000\n",
      "36324/36324 [==============================] - 24s 667us/step - loss: 0.0122 - val_loss: 0.2929\n",
      "Epoch 334/1000\n",
      "36324/36324 [==============================] - 25s 693us/step - loss: 0.0122 - val_loss: 0.2060\n",
      "Epoch 335/1000\n",
      "36324/36324 [==============================] - 25s 691us/step - loss: 0.0126 - val_loss: 0.3535\n",
      "Epoch 336/1000\n",
      "36324/36324 [==============================] - 23s 629us/step - loss: 0.0121 - val_loss: 0.3061\n",
      "Epoch 337/1000\n",
      "36324/36324 [==============================] - 25s 678us/step - loss: 0.0136 - val_loss: 0.2749\n",
      "Epoch 338/1000\n",
      "36324/36324 [==============================] - 25s 698us/step - loss: 0.0127 - val_loss: 0.2855\n",
      "Epoch 339/1000\n",
      "36324/36324 [==============================] - 24s 657us/step - loss: 0.0135 - val_loss: 0.2093\n",
      "Epoch 340/1000\n",
      "36324/36324 [==============================] - 25s 702us/step - loss: 0.0126 - val_loss: 0.2922\n",
      "Epoch 341/1000\n",
      "36324/36324 [==============================] - 25s 694us/step - loss: 0.0125 - val_loss: 0.2439\n",
      "Epoch 342/1000\n",
      "36324/36324 [==============================] - 25s 690us/step - loss: 0.0125 - val_loss: 0.3394\n",
      "Epoch 343/1000\n",
      "36324/36324 [==============================] - 26s 708us/step - loss: 0.0126 - val_loss: 0.3533\n",
      "Epoch 344/1000\n",
      "36324/36324 [==============================] - 24s 648us/step - loss: 0.0122 - val_loss: 0.3175\n",
      "Epoch 345/1000\n",
      "36324/36324 [==============================] - 27s 746us/step - loss: 0.0130 - val_loss: 0.3062\n",
      "Epoch 346/1000\n",
      "36324/36324 [==============================] - 28s 757us/step - loss: 0.0138 - val_loss: 0.2666\n",
      "Epoch 347/1000\n",
      "36324/36324 [==============================] - 26s 705us/step - loss: 0.0142 - val_loss: 0.2917\n",
      "Epoch 348/1000\n",
      "36324/36324 [==============================] - 27s 737us/step - loss: 0.0130 - val_loss: 0.2887\n",
      "Epoch 349/1000\n",
      "36324/36324 [==============================] - 25s 683us/step - loss: 0.0153 - val_loss: 0.3462\n",
      "Epoch 350/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 672s 18ms/step - loss: 0.0137 - val_loss: 0.3526\n",
      "Epoch 351/1000\n",
      "36324/36324 [==============================] - 66s 2ms/step - loss: 0.0131 - val_loss: 0.2673\n",
      "Epoch 352/1000\n",
      "36324/36324 [==============================] - 66s 2ms/step - loss: 0.0130 - val_loss: 0.3270\n",
      "Epoch 353/1000\n",
      "36324/36324 [==============================] - 29s 786us/step - loss: 0.0146 - val_loss: 0.4670\n",
      "Epoch 354/1000\n",
      "36324/36324 [==============================] - 26s 708us/step - loss: 0.0202 - val_loss: 0.3075\n",
      "Epoch 355/1000\n",
      "36324/36324 [==============================] - 27s 748us/step - loss: 0.0176 - val_loss: 0.2721\n",
      "Epoch 356/1000\n",
      "36324/36324 [==============================] - 24s 670us/step - loss: 0.0183 - val_loss: 0.3403\n",
      "Epoch 357/1000\n",
      "36324/36324 [==============================] - 26s 704us/step - loss: 0.0156 - val_loss: 0.3419\n",
      "Epoch 358/1000\n",
      "36324/36324 [==============================] - 27s 730us/step - loss: 0.0140 - val_loss: 0.2938\n",
      "Epoch 359/1000\n",
      "36324/36324 [==============================] - 24s 670us/step - loss: 0.0125 - val_loss: 0.3117\n",
      "Epoch 360/1000\n",
      "36324/36324 [==============================] - 25s 681us/step - loss: 0.0133 - val_loss: 0.3279\n",
      "Epoch 361/1000\n",
      "36324/36324 [==============================] - 25s 691us/step - loss: 0.0121 - val_loss: 0.2827\n",
      "Epoch 362/1000\n",
      "36324/36324 [==============================] - 26s 702us/step - loss: 0.0125 - val_loss: 0.2809\n",
      "Epoch 363/1000\n",
      "36324/36324 [==============================] - 24s 659us/step - loss: 0.0130 - val_loss: 0.2976\n",
      "Epoch 364/1000\n",
      "36324/36324 [==============================] - 25s 676us/step - loss: 0.0124 - val_loss: 0.3016\n",
      "Epoch 365/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36324/36324 [==============================] - 25s 688us/step - loss: 0.0126 - val_loss: 0.3055\n",
      "Epoch 366/1000\n",
      "36324/36324 [==============================] - 24s 668us/step - loss: 0.0125 - val_loss: 0.2608\n",
      "Epoch 367/1000\n",
      "36324/36324 [==============================] - 25s 690us/step - loss: 0.0116 - val_loss: 0.2738\n",
      "Epoch 368/1000\n",
      "36324/36324 [==============================] - 23s 631us/step - loss: 0.0130 - val_loss: 0.2997\n",
      "Epoch 369/1000\n",
      "36324/36324 [==============================] - 24s 657us/step - loss: 0.0117 - val_loss: 0.2971\n",
      "Epoch 370/1000\n",
      "36324/36324 [==============================] - 23s 627us/step - loss: 0.0119 - val_loss: 0.3238\n",
      "Epoch 371/1000\n",
      "36324/36324 [==============================] - 24s 671us/step - loss: 0.0119 - val_loss: 0.3196\n",
      "Epoch 372/1000\n",
      "36324/36324 [==============================] - 23s 639us/step - loss: 0.0143 - val_loss: 0.2846\n",
      "Epoch 373/1000\n",
      "36324/36324 [==============================] - 23s 622us/step - loss: 0.0123 - val_loss: 0.2739\n",
      "Epoch 374/1000\n",
      "36324/36324 [==============================] - 22s 603us/step - loss: 0.0120 - val_loss: 0.2790\n",
      "Epoch 375/1000\n",
      "36324/36324 [==============================] - 24s 655us/step - loss: 0.0119 - val_loss: 0.3465\n",
      "Epoch 376/1000\n",
      "36324/36324 [==============================] - 23s 639us/step - loss: 0.0123 - val_loss: 0.2833\n",
      "Epoch 377/1000\n",
      "36324/36324 [==============================] - 25s 694us/step - loss: 0.0125 - val_loss: 0.2866\n",
      "Epoch 378/1000\n",
      "36324/36324 [==============================] - 23s 641us/step - loss: 0.0133 - val_loss: 0.3017\n",
      "Epoch 379/1000\n",
      "36324/36324 [==============================] - 24s 663us/step - loss: 0.0121 - val_loss: 0.3208\n",
      "Epoch 380/1000\n",
      "36324/36324 [==============================] - 23s 622us/step - loss: 0.0123 - val_loss: 0.2645\n",
      "Epoch 381/1000\n",
      "36324/36324 [==============================] - 30s 823us/step - loss: 0.0123 - val_loss: 0.2834\n",
      "Epoch 382/1000\n",
      "36324/36324 [==============================] - 22s 610us/step - loss: 0.0124 - val_loss: 0.2812\n",
      "Epoch 383/1000\n",
      "36324/36324 [==============================] - 27s 750us/step - loss: 0.0122 - val_loss: 0.2949\n",
      "Epoch 384/1000\n",
      "36324/36324 [==============================] - 26s 729us/step - loss: 0.0134 - val_loss: 0.2932\n",
      "Epoch 385/1000\n",
      "36324/36324 [==============================] - 26s 712us/step - loss: 0.0116 - val_loss: 0.2888\n",
      "Epoch 386/1000\n",
      "36324/36324 [==============================] - 27s 736us/step - loss: 0.0126 - val_loss: 0.2971\n",
      "Epoch 387/1000\n",
      "36324/36324 [==============================] - 25s 683us/step - loss: 0.0121 - val_loss: 0.3177\n",
      "Epoch 388/1000\n",
      "36324/36324 [==============================] - 24s 653us/step - loss: 0.0116 - val_loss: 0.2774\n",
      "Epoch 389/1000\n",
      "36324/36324 [==============================] - 24s 671us/step - loss: 0.0122 - val_loss: 0.2781\n",
      "Epoch 390/1000\n",
      "36324/36324 [==============================] - 24s 651us/step - loss: 0.0116 - val_loss: 0.2859\n",
      "Epoch 391/1000\n",
      "36324/36324 [==============================] - 24s 648us/step - loss: 0.0114 - val_loss: 0.2803\n",
      "Epoch 392/1000\n",
      "36324/36324 [==============================] - 26s 708us/step - loss: 0.0126 - val_loss: 0.2901\n",
      "Epoch 393/1000\n",
      "36324/36324 [==============================] - 22s 615us/step - loss: 0.0133 - val_loss: 0.3936\n",
      "Epoch 394/1000\n",
      "36324/36324 [==============================] - 25s 675us/step - loss: 0.0125 - val_loss: 0.3058\n",
      "Epoch 395/1000\n",
      "36324/36324 [==============================] - 23s 633us/step - loss: 0.0140 - val_loss: 0.3040\n",
      "Epoch 396/1000\n",
      "36324/36324 [==============================] - 24s 661us/step - loss: 0.0123 - val_loss: 0.3812\n",
      "Epoch 397/1000\n",
      "36324/36324 [==============================] - 24s 656us/step - loss: 0.0126 - val_loss: 0.3907\n",
      "Epoch 398/1000\n",
      "36324/36324 [==============================] - 27s 740us/step - loss: 0.0121 - val_loss: 0.2958\n",
      "Epoch 399/1000\n",
      "36324/36324 [==============================] - 23s 636us/step - loss: 0.0129 - val_loss: 0.2606\n",
      "Epoch 400/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 658s 18ms/step - loss: 0.0118 - val_loss: 0.2805\n",
      "Epoch 401/1000\n",
      "36324/36324 [==============================] - 29s 804us/step - loss: 0.0121 - val_loss: 0.2573\n",
      "Epoch 402/1000\n",
      "36324/36324 [==============================] - 35s 964us/step - loss: 0.0125 - val_loss: 0.2776\n",
      "Epoch 403/1000\n",
      "36324/36324 [==============================] - 40s 1ms/step - loss: 0.0123 - val_loss: 0.4626\n",
      "Epoch 404/1000\n",
      "36324/36324 [==============================] - 26s 729us/step - loss: 0.0126 - val_loss: 0.4308\n",
      "Epoch 405/1000\n",
      "36324/36324 [==============================] - 25s 675us/step - loss: 0.0120 - val_loss: 0.2731\n",
      "Epoch 406/1000\n",
      "36324/36324 [==============================] - 33s 909us/step - loss: 0.0122 - val_loss: 0.2926\n",
      "Epoch 407/1000\n",
      "36324/36324 [==============================] - 24s 652us/step - loss: 0.0119 - val_loss: 0.3342\n",
      "Epoch 408/1000\n",
      "36324/36324 [==============================] - 24s 663us/step - loss: 0.0119 - val_loss: 0.2816\n",
      "Epoch 409/1000\n",
      "36324/36324 [==============================] - 25s 690us/step - loss: 0.0120 - val_loss: 0.3828\n",
      "Epoch 410/1000\n",
      "36324/36324 [==============================] - 23s 636us/step - loss: 0.0125 - val_loss: 0.3339\n",
      "Epoch 411/1000\n",
      "36324/36324 [==============================] - 23s 646us/step - loss: 0.0120 - val_loss: 0.2309\n",
      "Epoch 412/1000\n",
      "36324/36324 [==============================] - 24s 656us/step - loss: 0.0124 - val_loss: 0.3564\n",
      "Epoch 413/1000\n",
      "36324/36324 [==============================] - 24s 656us/step - loss: 0.0115 - val_loss: 0.2607\n",
      "Epoch 414/1000\n",
      "36324/36324 [==============================] - 24s 649us/step - loss: 0.0120 - val_loss: 0.2626\n",
      "Epoch 415/1000\n",
      "36324/36324 [==============================] - 25s 680us/step - loss: 0.0116 - val_loss: 0.2856\n",
      "Epoch 416/1000\n",
      "36324/36324 [==============================] - 25s 698us/step - loss: 0.0118 - val_loss: 0.2648\n",
      "Epoch 417/1000\n",
      "36324/36324 [==============================] - 24s 651us/step - loss: 0.0128 - val_loss: 0.3617\n",
      "Epoch 418/1000\n",
      "36324/36324 [==============================] - 23s 642us/step - loss: 0.0123 - val_loss: 0.3329\n",
      "Epoch 419/1000\n",
      "36324/36324 [==============================] - 24s 673us/step - loss: 0.0175 - val_loss: 0.2565\n",
      "Epoch 420/1000\n",
      "36324/36324 [==============================] - 23s 634us/step - loss: 0.0179 - val_loss: 0.3250\n",
      "Epoch 421/1000\n",
      "36324/36324 [==============================] - 24s 674us/step - loss: 0.0146 - val_loss: 0.2968\n",
      "Epoch 422/1000\n",
      "36324/36324 [==============================] - 24s 670us/step - loss: 0.0141 - val_loss: 0.2764\n",
      "Epoch 423/1000\n",
      "36324/36324 [==============================] - 22s 610us/step - loss: 0.0127 - val_loss: 0.2634\n",
      "Epoch 424/1000\n",
      "36324/36324 [==============================] - 23s 632us/step - loss: 0.0127 - val_loss: 0.3471\n",
      "Epoch 425/1000\n",
      "36324/36324 [==============================] - 22s 615us/step - loss: 0.0129 - val_loss: 0.3134\n",
      "Epoch 426/1000\n",
      "36324/36324 [==============================] - 25s 679us/step - loss: 0.0122 - val_loss: 0.2662\n",
      "Epoch 427/1000\n",
      "36324/36324 [==============================] - 23s 631us/step - loss: 0.0130 - val_loss: 0.3294\n",
      "Epoch 428/1000\n",
      "36324/36324 [==============================] - 22s 612us/step - loss: 0.0113 - val_loss: 0.2501\n",
      "Epoch 429/1000\n",
      "36324/36324 [==============================] - 24s 659us/step - loss: 0.0119 - val_loss: 0.3241\n",
      "Epoch 430/1000\n",
      "36324/36324 [==============================] - 24s 669us/step - loss: 0.0112 - val_loss: 0.3429\n",
      "Epoch 431/1000\n",
      "36324/36324 [==============================] - 24s 655us/step - loss: 0.0120 - val_loss: 0.2799\n",
      "Epoch 432/1000\n",
      "36324/36324 [==============================] - 25s 680us/step - loss: 0.0126 - val_loss: 0.3119\n",
      "Epoch 433/1000\n",
      "36324/36324 [==============================] - 23s 629us/step - loss: 0.0118 - val_loss: 0.2674\n",
      "Epoch 434/1000\n",
      "36324/36324 [==============================] - 26s 710us/step - loss: 0.0123 - val_loss: 0.3875\n",
      "Epoch 435/1000\n",
      "36324/36324 [==============================] - 27s 732us/step - loss: 0.0122 - val_loss: 0.3165\n",
      "Epoch 436/1000\n",
      "36324/36324 [==============================] - 26s 709us/step - loss: 0.0116 - val_loss: 0.2818\n",
      "Epoch 437/1000\n",
      "36324/36324 [==============================] - 24s 652us/step - loss: 0.0111 - val_loss: 0.3708\n",
      "Epoch 438/1000\n",
      "36324/36324 [==============================] - 22s 604us/step - loss: 0.0113 - val_loss: 0.3834\n",
      "Epoch 439/1000\n",
      "36324/36324 [==============================] - 22s 599us/step - loss: 0.0124 - val_loss: 0.3280\n",
      "Epoch 440/1000\n",
      "36324/36324 [==============================] - 23s 647us/step - loss: 0.0119 - val_loss: 0.3353\n",
      "Epoch 441/1000\n",
      "36324/36324 [==============================] - 24s 649us/step - loss: 0.0122 - val_loss: 0.2890\n",
      "Epoch 442/1000\n",
      "36324/36324 [==============================] - 24s 673us/step - loss: 0.0110 - val_loss: 0.3242\n",
      "Epoch 443/1000\n",
      "36324/36324 [==============================] - 25s 694us/step - loss: 0.0112 - val_loss: 0.2334\n",
      "Epoch 444/1000\n",
      "36324/36324 [==============================] - 23s 628us/step - loss: 0.0118 - val_loss: 0.3462\n",
      "Epoch 445/1000\n",
      "36324/36324 [==============================] - 24s 652us/step - loss: 0.0124 - val_loss: 0.3372\n",
      "Epoch 446/1000\n",
      "36324/36324 [==============================] - 23s 639us/step - loss: 0.0122 - val_loss: 0.3023\n",
      "Epoch 447/1000\n",
      "36324/36324 [==============================] - 24s 653us/step - loss: 0.0112 - val_loss: 0.3181\n",
      "Epoch 448/1000\n",
      "36324/36324 [==============================] - 23s 633us/step - loss: 0.0126 - val_loss: 0.3075\n",
      "Epoch 449/1000\n",
      "36324/36324 [==============================] - 23s 625us/step - loss: 0.0113 - val_loss: 0.3175\n",
      "Epoch 450/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 645s 18ms/step - loss: 0.0117 - val_loss: 0.2823\n",
      "Epoch 451/1000\n",
      "36324/36324 [==============================] - 26s 702us/step - loss: 0.0120 - val_loss: 0.2687\n",
      "Epoch 452/1000\n",
      "36324/36324 [==============================] - 26s 718us/step - loss: 0.0115 - val_loss: 0.4371\n",
      "Epoch 453/1000\n",
      "36324/36324 [==============================] - 22s 616us/step - loss: 0.0118 - val_loss: 0.3615\n",
      "Epoch 454/1000\n",
      "36324/36324 [==============================] - 22s 616us/step - loss: 0.0111 - val_loss: 0.2796\n",
      "Epoch 455/1000\n",
      "36324/36324 [==============================] - 24s 671us/step - loss: 0.0121 - val_loss: 0.3185\n",
      "Epoch 456/1000\n",
      "36324/36324 [==============================] - 25s 682us/step - loss: 0.0116 - val_loss: 0.2977\n",
      "Epoch 457/1000\n",
      "36324/36324 [==============================] - 25s 694us/step - loss: 0.0118 - val_loss: 0.2694\n",
      "Epoch 458/1000\n",
      "36324/36324 [==============================] - 29s 794us/step - loss: 0.0120 - val_loss: 0.2695\n",
      "Epoch 459/1000\n",
      "36324/36324 [==============================] - 23s 633us/step - loss: 0.0125 - val_loss: 0.3446\n",
      "Epoch 460/1000\n",
      "36324/36324 [==============================] - 22s 612us/step - loss: 0.0125 - val_loss: 0.4277\n",
      "Epoch 461/1000\n",
      "36324/36324 [==============================] - 23s 631us/step - loss: 0.0128 - val_loss: 0.3127\n",
      "Epoch 462/1000\n",
      "36324/36324 [==============================] - 23s 623us/step - loss: 0.0124 - val_loss: 0.2912\n",
      "Epoch 463/1000\n",
      "36324/36324 [==============================] - 23s 641us/step - loss: 0.0125 - val_loss: 0.3030\n",
      "Epoch 464/1000\n",
      "36324/36324 [==============================] - 24s 658us/step - loss: 0.0142 - val_loss: 0.3157\n",
      "Epoch 465/1000\n",
      "36324/36324 [==============================] - 24s 657us/step - loss: 0.0151 - val_loss: 0.3123\n",
      "Epoch 466/1000\n",
      "36324/36324 [==============================] - 25s 675us/step - loss: 0.0122 - val_loss: 0.3014\n",
      "Epoch 467/1000\n",
      "36324/36324 [==============================] - 25s 685us/step - loss: 0.0140 - val_loss: 0.2866\n",
      "Epoch 468/1000\n",
      "36324/36324 [==============================] - 26s 708us/step - loss: 0.0134 - val_loss: 0.2840\n",
      "Epoch 469/1000\n",
      "36324/36324 [==============================] - 23s 629us/step - loss: 0.0153 - val_loss: 0.2746\n",
      "Epoch 470/1000\n",
      "36324/36324 [==============================] - 23s 641us/step - loss: 0.0155 - val_loss: 0.3201\n",
      "Epoch 471/1000\n",
      "36324/36324 [==============================] - 24s 658us/step - loss: 0.0142 - val_loss: 0.3908\n",
      "Epoch 472/1000\n",
      "36324/36324 [==============================] - 23s 637us/step - loss: 0.0125 - val_loss: 0.2929\n",
      "Epoch 473/1000\n",
      "36324/36324 [==============================] - 22s 602us/step - loss: 0.0123 - val_loss: 0.2794\n",
      "Epoch 474/1000\n",
      "36324/36324 [==============================] - 23s 640us/step - loss: 0.0112 - val_loss: 0.2955\n",
      "Epoch 475/1000\n",
      "36324/36324 [==============================] - 22s 615us/step - loss: 0.0130 - val_loss: 0.3251\n",
      "Epoch 476/1000\n",
      "36324/36324 [==============================] - 22s 611us/step - loss: 0.0119 - val_loss: 0.3364\n",
      "Epoch 477/1000\n",
      "36324/36324 [==============================] - 22s 603us/step - loss: 0.0114 - val_loss: 0.2714\n",
      "Epoch 478/1000\n",
      "36324/36324 [==============================] - 23s 631us/step - loss: 0.0122 - val_loss: 0.3113\n",
      "Epoch 479/1000\n",
      "36324/36324 [==============================] - 24s 668us/step - loss: 0.0120 - val_loss: 0.3177\n",
      "Epoch 480/1000\n",
      "36324/36324 [==============================] - 25s 699us/step - loss: 0.0115 - val_loss: 0.3100\n",
      "Epoch 481/1000\n",
      "36324/36324 [==============================] - 25s 692us/step - loss: 0.0121 - val_loss: 0.2694\n",
      "Epoch 482/1000\n",
      "36324/36324 [==============================] - 24s 649us/step - loss: 0.0115 - val_loss: 0.3461\n",
      "Epoch 483/1000\n",
      "36324/36324 [==============================] - 22s 594us/step - loss: 0.0104 - val_loss: 0.3078\n",
      "Epoch 484/1000\n",
      "36324/36324 [==============================] - 22s 606us/step - loss: 0.0110 - val_loss: 0.2875\n",
      "Epoch 485/1000\n",
      "36324/36324 [==============================] - 24s 651us/step - loss: 0.0112 - val_loss: 0.3237\n",
      "Epoch 486/1000\n",
      "36324/36324 [==============================] - 23s 620us/step - loss: 0.0121 - val_loss: 0.3092\n",
      "Epoch 487/1000\n",
      "36324/36324 [==============================] - 20s 562us/step - loss: 0.0118 - val_loss: 0.3333\n",
      "Epoch 488/1000\n",
      "36324/36324 [==============================] - 22s 596us/step - loss: 0.0117 - val_loss: 0.2900\n",
      "Epoch 489/1000\n",
      "36324/36324 [==============================] - 24s 654us/step - loss: 0.0120 - val_loss: 0.3219\n",
      "Epoch 490/1000\n",
      "36324/36324 [==============================] - 22s 604us/step - loss: 0.0126 - val_loss: 0.3224\n",
      "Epoch 491/1000\n",
      "36324/36324 [==============================] - 23s 621us/step - loss: 0.0113 - val_loss: 0.3134\n",
      "Epoch 492/1000\n",
      "36324/36324 [==============================] - 22s 613us/step - loss: 0.0111 - val_loss: 0.3617\n",
      "Epoch 493/1000\n",
      "36324/36324 [==============================] - 23s 633us/step - loss: 0.0114 - val_loss: 0.3211\n",
      "Epoch 494/1000\n",
      "36324/36324 [==============================] - 22s 615us/step - loss: 0.0116 - val_loss: 0.3805\n",
      "Epoch 495/1000\n",
      "36324/36324 [==============================] - 24s 656us/step - loss: 0.0114 - val_loss: 0.2676\n",
      "Epoch 496/1000\n",
      "36324/36324 [==============================] - 25s 683us/step - loss: 0.0111 - val_loss: 0.4083\n",
      "Epoch 497/1000\n",
      "36324/36324 [==============================] - 25s 688us/step - loss: 0.0120 - val_loss: 0.2920\n",
      "Epoch 498/1000\n",
      "36324/36324 [==============================] - 21s 590us/step - loss: 0.0123 - val_loss: 0.3591\n",
      "Epoch 499/1000\n",
      "36324/36324 [==============================] - 21s 590us/step - loss: 0.0121 - val_loss: 0.2449\n",
      "Epoch 500/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 624s 17ms/step - loss: 0.0116 - val_loss: 0.2416\n",
      "Epoch 501/1000\n",
      "36324/36324 [==============================] - 24s 650us/step - loss: 0.0123 - val_loss: 0.3164\n",
      "Epoch 502/1000\n",
      "36324/36324 [==============================] - 32s 868us/step - loss: 0.0135 - val_loss: 0.2445\n",
      "Epoch 503/1000\n",
      "36324/36324 [==============================] - 24s 657us/step - loss: 0.0126 - val_loss: 0.2623\n",
      "Epoch 504/1000\n",
      "36324/36324 [==============================] - 24s 664us/step - loss: 0.0122 - val_loss: 0.3373\n",
      "Epoch 505/1000\n",
      "36324/36324 [==============================] - 22s 605us/step - loss: 0.0120 - val_loss: 0.3177\n",
      "Epoch 506/1000\n",
      "36324/36324 [==============================] - 24s 674us/step - loss: 0.0125 - val_loss: 0.2732\n",
      "Epoch 507/1000\n",
      "36324/36324 [==============================] - 24s 650us/step - loss: 0.0157 - val_loss: 0.2834\n",
      "Epoch 508/1000\n",
      "36324/36324 [==============================] - 22s 608us/step - loss: 0.0134 - val_loss: 0.2732\n",
      "Epoch 509/1000\n",
      "36324/36324 [==============================] - 29s 798us/step - loss: 0.0125 - val_loss: 0.2961\n",
      "Epoch 510/1000\n",
      "36324/36324 [==============================] - 20s 564us/step - loss: 0.0118 - val_loss: 0.2913\n",
      "Epoch 511/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36324/36324 [==============================] - 22s 605us/step - loss: 0.0121 - val_loss: 0.2829\n",
      "Epoch 512/1000\n",
      "36324/36324 [==============================] - 22s 611us/step - loss: 0.0126 - val_loss: 0.2850\n",
      "Epoch 513/1000\n",
      "36324/36324 [==============================] - 20s 559us/step - loss: 0.0143 - val_loss: 0.2832\n",
      "Epoch 514/1000\n",
      "36324/36324 [==============================] - 21s 577us/step - loss: 0.0128 - val_loss: 0.2680\n",
      "Epoch 515/1000\n",
      "36324/36324 [==============================] - 22s 599us/step - loss: 0.0142 - val_loss: 0.2697\n",
      "Epoch 516/1000\n",
      "36324/36324 [==============================] - 24s 654us/step - loss: 0.0132 - val_loss: 0.3168\n",
      "Epoch 517/1000\n",
      "36324/36324 [==============================] - 22s 613us/step - loss: 0.0113 - val_loss: 0.3520\n",
      "Epoch 518/1000\n",
      "36324/36324 [==============================] - 23s 647us/step - loss: 0.0114 - val_loss: 0.2925\n",
      "Epoch 519/1000\n",
      "36324/36324 [==============================] - 22s 619us/step - loss: 0.0113 - val_loss: 0.3105\n",
      "Epoch 520/1000\n",
      "36324/36324 [==============================] - 21s 587us/step - loss: 0.0110 - val_loss: 0.3744\n",
      "Epoch 521/1000\n",
      "36324/36324 [==============================] - 21s 588us/step - loss: 0.0110 - val_loss: 0.3308\n",
      "Epoch 522/1000\n",
      "36324/36324 [==============================] - 24s 655us/step - loss: 0.0117 - val_loss: 0.3101\n",
      "Epoch 523/1000\n",
      "36324/36324 [==============================] - 22s 601us/step - loss: 0.0122 - val_loss: 0.2658\n",
      "Epoch 524/1000\n",
      "36324/36324 [==============================] - 21s 584us/step - loss: 0.0122 - val_loss: 0.3450\n",
      "Epoch 525/1000\n",
      "36324/36324 [==============================] - 23s 626us/step - loss: 0.0117 - val_loss: 0.3215\n",
      "Epoch 526/1000\n",
      "36324/36324 [==============================] - 22s 607us/step - loss: 0.0117 - val_loss: 0.2265\n",
      "Epoch 527/1000\n",
      "36324/36324 [==============================] - 22s 613us/step - loss: 0.0116 - val_loss: 0.2822\n",
      "Epoch 528/1000\n",
      "36324/36324 [==============================] - 22s 597us/step - loss: 0.0114 - val_loss: 0.3576\n",
      "Epoch 529/1000\n",
      "36324/36324 [==============================] - 21s 591us/step - loss: 0.0115 - val_loss: 0.4681\n",
      "Epoch 530/1000\n",
      "36324/36324 [==============================] - 21s 585us/step - loss: 0.0122 - val_loss: 0.3687\n",
      "Epoch 531/1000\n",
      "36324/36324 [==============================] - 25s 689us/step - loss: 0.0114 - val_loss: 0.3273\n",
      "Epoch 532/1000\n",
      "36324/36324 [==============================] - 29s 796us/step - loss: 0.0110 - val_loss: 0.3524\n",
      "Epoch 533/1000\n",
      "36324/36324 [==============================] - 25s 685us/step - loss: 0.0108 - val_loss: 0.3650\n",
      "Epoch 534/1000\n",
      "36324/36324 [==============================] - 24s 666us/step - loss: 0.0122 - val_loss: 0.3299\n",
      "Epoch 535/1000\n",
      "36324/36324 [==============================] - 24s 661us/step - loss: 0.0122 - val_loss: 0.2847\n",
      "Epoch 536/1000\n",
      "36324/36324 [==============================] - 25s 678us/step - loss: 0.0124 - val_loss: 0.2864\n",
      "Epoch 537/1000\n",
      "36324/36324 [==============================] - 25s 693us/step - loss: 0.0109 - val_loss: 0.3495\n",
      "Epoch 538/1000\n",
      "36324/36324 [==============================] - 24s 666us/step - loss: 0.0114 - val_loss: 0.3590\n",
      "Epoch 539/1000\n",
      "36324/36324 [==============================] - 24s 670us/step - loss: 0.0123 - val_loss: 0.2915\n",
      "Epoch 540/1000\n",
      "36324/36324 [==============================] - 23s 628us/step - loss: 0.0114 - val_loss: 0.3670\n",
      "Epoch 541/1000\n",
      "36324/36324 [==============================] - 23s 645us/step - loss: 0.0118 - val_loss: 0.3406\n",
      "Epoch 542/1000\n",
      "36324/36324 [==============================] - 26s 710us/step - loss: 0.0134 - val_loss: 0.3107\n",
      "Epoch 543/1000\n",
      "36324/36324 [==============================] - 23s 627us/step - loss: 0.0107 - val_loss: 0.4088\n",
      "Epoch 544/1000\n",
      "36324/36324 [==============================] - 22s 619us/step - loss: 0.0112 - val_loss: 0.3447\n",
      "Epoch 545/1000\n",
      "36324/36324 [==============================] - 24s 663us/step - loss: 0.0108 - val_loss: 0.3613\n",
      "Epoch 546/1000\n",
      "36324/36324 [==============================] - 24s 648us/step - loss: 0.0123 - val_loss: 0.2429\n",
      "Epoch 547/1000\n",
      "36324/36324 [==============================] - 25s 700us/step - loss: 0.0113 - val_loss: 0.2668\n",
      "Epoch 548/1000\n",
      "36324/36324 [==============================] - 23s 637us/step - loss: 0.0118 - val_loss: 0.2633\n",
      "Epoch 549/1000\n",
      "36324/36324 [==============================] - 74s 2ms/step - loss: 0.0108 - val_loss: 0.3050\n",
      "Epoch 550/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 669s 18ms/step - loss: 0.0111 - val_loss: 0.3566\n",
      "Epoch 551/1000\n",
      "36324/36324 [==============================] - 27s 746us/step - loss: 0.0127 - val_loss: 0.3796\n",
      "Epoch 552/1000\n",
      "36324/36324 [==============================] - 28s 768us/step - loss: 0.0116 - val_loss: 0.3337\n",
      "Epoch 553/1000\n",
      "36324/36324 [==============================] - 27s 731us/step - loss: 0.0109 - val_loss: 0.3205\n",
      "Epoch 554/1000\n",
      "36324/36324 [==============================] - 24s 666us/step - loss: 0.0113 - val_loss: 0.2655\n",
      "Epoch 555/1000\n",
      "36324/36324 [==============================] - 24s 656us/step - loss: 0.0114 - val_loss: 0.3081\n",
      "Epoch 556/1000\n",
      "36324/36324 [==============================] - 25s 694us/step - loss: 0.0115 - val_loss: 0.3546\n",
      "Epoch 557/1000\n",
      "36324/36324 [==============================] - 26s 716us/step - loss: 0.0126 - val_loss: 0.2974\n",
      "Epoch 558/1000\n",
      "36324/36324 [==============================] - 25s 700us/step - loss: 0.0120 - val_loss: 0.3484\n",
      "Epoch 559/1000\n",
      "36324/36324 [==============================] - 24s 665us/step - loss: 0.0120 - val_loss: 0.2886\n",
      "Epoch 560/1000\n",
      "36324/36324 [==============================] - 24s 663us/step - loss: 0.0109 - val_loss: 0.3063\n",
      "Epoch 561/1000\n",
      "36324/36324 [==============================] - 22s 606us/step - loss: 0.0114 - val_loss: 0.3381\n",
      "Epoch 562/1000\n",
      "36324/36324 [==============================] - 24s 672us/step - loss: 0.0108 - val_loss: 0.3796\n",
      "Epoch 563/1000\n",
      "36324/36324 [==============================] - 25s 695us/step - loss: 0.0114 - val_loss: 0.3772\n",
      "Epoch 564/1000\n",
      "36324/36324 [==============================] - 25s 690us/step - loss: 0.0111 - val_loss: 0.3908\n",
      "Epoch 565/1000\n",
      "36324/36324 [==============================] - 25s 701us/step - loss: 0.0116 - val_loss: 0.4111\n",
      "Epoch 566/1000\n",
      "36324/36324 [==============================] - 25s 681us/step - loss: 0.0104 - val_loss: 0.3452\n",
      "Epoch 567/1000\n",
      "36324/36324 [==============================] - 26s 722us/step - loss: 0.0112 - val_loss: 0.4079\n",
      "Epoch 568/1000\n",
      "36324/36324 [==============================] - 25s 682us/step - loss: 0.0118 - val_loss: 0.3822\n",
      "Epoch 569/1000\n",
      "36324/36324 [==============================] - 24s 667us/step - loss: 0.0116 - val_loss: 0.3386\n",
      "Epoch 570/1000\n",
      "36324/36324 [==============================] - 24s 649us/step - loss: 0.0111 - val_loss: 0.3816\n",
      "Epoch 571/1000\n",
      "36324/36324 [==============================] - 23s 640us/step - loss: 0.0115 - val_loss: 0.3415\n",
      "Epoch 572/1000\n",
      "36324/36324 [==============================] - 25s 690us/step - loss: 0.0115 - val_loss: 0.2805\n",
      "Epoch 573/1000\n",
      "36324/36324 [==============================] - 25s 689us/step - loss: 0.0107 - val_loss: 0.3100\n",
      "Epoch 574/1000\n",
      "36324/36324 [==============================] - 24s 666us/step - loss: 0.0118 - val_loss: 0.4719\n",
      "Epoch 575/1000\n",
      "36324/36324 [==============================] - 23s 624us/step - loss: 0.0125 - val_loss: 0.3392\n",
      "Epoch 576/1000\n",
      "36324/36324 [==============================] - 24s 654us/step - loss: 0.0118 - val_loss: 0.2972\n",
      "Epoch 577/1000\n",
      "36324/36324 [==============================] - 25s 687us/step - loss: 0.0142 - val_loss: 0.3845\n",
      "Epoch 578/1000\n",
      "36324/36324 [==============================] - 25s 698us/step - loss: 0.0137 - val_loss: 0.2891\n",
      "Epoch 579/1000\n",
      "36324/36324 [==============================] - 25s 687us/step - loss: 0.0127 - val_loss: 0.3397\n",
      "Epoch 580/1000\n",
      "36324/36324 [==============================] - 24s 667us/step - loss: 0.0122 - val_loss: 0.3450\n",
      "Epoch 581/1000\n",
      "36324/36324 [==============================] - 24s 665us/step - loss: 0.0118 - val_loss: 0.3343\n",
      "Epoch 582/1000\n",
      "36324/36324 [==============================] - 25s 694us/step - loss: 0.0121 - val_loss: 0.3198\n",
      "Epoch 583/1000\n",
      "36324/36324 [==============================] - 26s 719us/step - loss: 0.0112 - val_loss: 0.3614\n",
      "Epoch 584/1000\n",
      "36324/36324 [==============================] - 24s 669us/step - loss: 0.0109 - val_loss: 0.3346\n",
      "Epoch 585/1000\n",
      "36324/36324 [==============================] - 25s 679us/step - loss: 0.0116 - val_loss: 0.3634\n",
      "Epoch 586/1000\n",
      "36324/36324 [==============================] - 25s 675us/step - loss: 0.0109 - val_loss: 0.3245\n",
      "Epoch 587/1000\n",
      "36324/36324 [==============================] - 25s 685us/step - loss: 0.0106 - val_loss: 0.3350\n",
      "Epoch 588/1000\n",
      "36324/36324 [==============================] - 26s 708us/step - loss: 0.0113 - val_loss: 0.3371\n",
      "Epoch 589/1000\n",
      "36324/36324 [==============================] - 27s 753us/step - loss: 0.0117 - val_loss: 0.2920\n",
      "Epoch 590/1000\n",
      "36324/36324 [==============================] - 26s 704us/step - loss: 0.0109 - val_loss: 0.2871\n",
      "Epoch 591/1000\n",
      "36324/36324 [==============================] - 23s 640us/step - loss: 0.0117 - val_loss: 0.3558\n",
      "Epoch 592/1000\n",
      "36324/36324 [==============================] - 22s 608us/step - loss: 0.0109 - val_loss: 0.3902\n",
      "Epoch 593/1000\n",
      "36324/36324 [==============================] - 25s 689us/step - loss: 0.0123 - val_loss: 0.3407\n",
      "Epoch 594/1000\n",
      "36324/36324 [==============================] - 24s 671us/step - loss: 0.0122 - val_loss: 0.3646\n",
      "Epoch 595/1000\n",
      "36324/36324 [==============================] - 25s 686us/step - loss: 0.0110 - val_loss: 0.3135\n",
      "Epoch 596/1000\n",
      "36324/36324 [==============================] - 24s 661us/step - loss: 0.0113 - val_loss: 0.4464\n",
      "Epoch 597/1000\n",
      "36324/36324 [==============================] - 24s 654us/step - loss: 0.0128 - val_loss: 0.3555\n",
      "Epoch 598/1000\n",
      "36324/36324 [==============================] - 25s 688us/step - loss: 0.0109 - val_loss: 0.3173\n",
      "Epoch 599/1000\n",
      "36324/36324 [==============================] - 24s 663us/step - loss: 0.0108 - val_loss: 0.4523\n",
      "Epoch 600/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 641s 18ms/step - loss: 0.0117 - val_loss: 0.3976\n",
      "Epoch 601/1000\n",
      "36324/36324 [==============================] - 26s 728us/step - loss: 0.0123 - val_loss: 0.3548\n",
      "Epoch 602/1000\n",
      "36324/36324 [==============================] - 26s 703us/step - loss: 0.0113 - val_loss: 0.3389\n",
      "Epoch 603/1000\n",
      "36324/36324 [==============================] - 24s 668us/step - loss: 0.0105 - val_loss: 0.3344\n",
      "Epoch 604/1000\n",
      "36324/36324 [==============================] - 25s 694us/step - loss: 0.0127 - val_loss: 0.3711\n",
      "Epoch 605/1000\n",
      "36324/36324 [==============================] - 27s 739us/step - loss: 0.0116 - val_loss: 0.3754\n",
      "Epoch 606/1000\n",
      "36324/36324 [==============================] - 27s 754us/step - loss: 0.0131 - val_loss: 0.3287\n",
      "Epoch 607/1000\n",
      "36324/36324 [==============================] - 27s 736us/step - loss: 0.0120 - val_loss: 0.3115\n",
      "Epoch 608/1000\n",
      "36324/36324 [==============================] - 23s 646us/step - loss: 0.0109 - val_loss: 0.3417\n",
      "Epoch 609/1000\n",
      "36324/36324 [==============================] - 24s 650us/step - loss: 0.0122 - val_loss: 0.3108\n",
      "Epoch 610/1000\n",
      "36324/36324 [==============================] - 25s 681us/step - loss: 0.0126 - val_loss: 0.3250\n",
      "Epoch 611/1000\n",
      "36324/36324 [==============================] - 23s 620us/step - loss: 0.0114 - val_loss: 0.3850\n",
      "Epoch 612/1000\n",
      "36324/36324 [==============================] - 23s 643us/step - loss: 0.0124 - val_loss: 0.2983\n",
      "Epoch 613/1000\n",
      "36324/36324 [==============================] - 22s 607us/step - loss: 0.0111 - val_loss: 0.3608\n",
      "Epoch 614/1000\n",
      "36324/36324 [==============================] - 23s 643us/step - loss: 0.0104 - val_loss: 0.3918\n",
      "Epoch 615/1000\n",
      "36324/36324 [==============================] - 23s 622us/step - loss: 0.0108 - val_loss: 0.3390\n",
      "Epoch 616/1000\n",
      "36324/36324 [==============================] - 22s 592us/step - loss: 0.0117 - val_loss: 0.2801\n",
      "Epoch 617/1000\n",
      "36324/36324 [==============================] - 23s 643us/step - loss: 0.0122 - val_loss: 0.2599\n",
      "Epoch 618/1000\n",
      "36324/36324 [==============================] - 24s 652us/step - loss: 0.0116 - val_loss: 0.3560\n",
      "Epoch 619/1000\n",
      "36324/36324 [==============================] - 24s 658us/step - loss: 0.0109 - val_loss: 0.3091\n",
      "Epoch 620/1000\n",
      "36324/36324 [==============================] - 22s 601us/step - loss: 0.0122 - val_loss: 0.3300\n",
      "Epoch 621/1000\n",
      "36324/36324 [==============================] - 22s 612us/step - loss: 0.0113 - val_loss: 0.5059\n",
      "Epoch 622/1000\n",
      "36324/36324 [==============================] - 40s 1ms/step - loss: 0.0119 - val_loss: 0.4951\n",
      "Epoch 623/1000\n",
      "36324/36324 [==============================] - 60s 2ms/step - loss: 0.0114 - val_loss: 0.4218\n",
      "Epoch 624/1000\n",
      "36324/36324 [==============================] - 27s 732us/step - loss: 0.0109 - val_loss: 0.3488\n",
      "Epoch 625/1000\n",
      "36324/36324 [==============================] - 25s 700us/step - loss: 0.0116 - val_loss: 0.3279\n",
      "Epoch 626/1000\n",
      "36324/36324 [==============================] - 27s 736us/step - loss: 0.0105 - val_loss: 0.3870\n",
      "Epoch 627/1000\n",
      "36324/36324 [==============================] - 26s 704us/step - loss: 0.0115 - val_loss: 0.3484\n",
      "Epoch 628/1000\n",
      "36324/36324 [==============================] - 25s 684us/step - loss: 0.0130 - val_loss: 0.4522\n",
      "Epoch 629/1000\n",
      "36324/36324 [==============================] - 24s 670us/step - loss: 0.0120 - val_loss: 0.3992\n",
      "Epoch 630/1000\n",
      "36324/36324 [==============================] - 25s 677us/step - loss: 0.0114 - val_loss: 0.3872\n",
      "Epoch 631/1000\n",
      "36324/36324 [==============================] - 24s 656us/step - loss: 0.0119 - val_loss: 0.3986\n",
      "Epoch 632/1000\n",
      "36324/36324 [==============================] - 26s 704us/step - loss: 0.0113 - val_loss: 0.3658\n",
      "Epoch 633/1000\n",
      "36324/36324 [==============================] - 22s 600us/step - loss: 0.0116 - val_loss: 0.3366\n",
      "Epoch 634/1000\n",
      "36324/36324 [==============================] - 22s 614us/step - loss: 0.0120 - val_loss: 0.2908\n",
      "Epoch 635/1000\n",
      "36324/36324 [==============================] - 24s 655us/step - loss: 0.0122 - val_loss: 0.3212\n",
      "Epoch 636/1000\n",
      "36324/36324 [==============================] - 24s 664us/step - loss: 0.0125 - val_loss: 0.3679\n",
      "Epoch 637/1000\n",
      "36324/36324 [==============================] - 25s 679us/step - loss: 0.0131 - val_loss: 0.2810\n",
      "Epoch 638/1000\n",
      "36324/36324 [==============================] - 25s 701us/step - loss: 0.0162 - val_loss: 0.4049\n",
      "Epoch 639/1000\n",
      "36324/36324 [==============================] - 24s 664us/step - loss: 0.0195 - val_loss: 0.3316\n",
      "Epoch 640/1000\n",
      "36324/36324 [==============================] - 25s 700us/step - loss: 0.0175 - val_loss: 0.3489\n",
      "Epoch 641/1000\n",
      "36324/36324 [==============================] - 24s 654us/step - loss: 0.0142 - val_loss: 0.3645\n",
      "Epoch 642/1000\n",
      "36324/36324 [==============================] - 24s 653us/step - loss: 0.0113 - val_loss: 0.4270\n",
      "Epoch 643/1000\n",
      "36324/36324 [==============================] - 24s 666us/step - loss: 0.0125 - val_loss: 0.3221\n",
      "Epoch 644/1000\n",
      "36324/36324 [==============================] - 24s 657us/step - loss: 0.0133 - val_loss: 0.4003\n",
      "Epoch 645/1000\n",
      "36324/36324 [==============================] - 23s 633us/step - loss: 0.0121 - val_loss: 0.3572\n",
      "Epoch 646/1000\n",
      "36324/36324 [==============================] - 25s 675us/step - loss: 0.0107 - val_loss: 0.3329\n",
      "Epoch 647/1000\n",
      "36324/36324 [==============================] - 23s 630us/step - loss: 0.0109 - val_loss: 0.3427\n",
      "Epoch 648/1000\n",
      "36324/36324 [==============================] - 23s 625us/step - loss: 0.0111 - val_loss: 0.3372\n",
      "Epoch 649/1000\n",
      "36324/36324 [==============================] - 27s 733us/step - loss: 0.0113 - val_loss: 0.3303\n",
      "Epoch 650/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 637s 18ms/step - loss: 0.0109 - val_loss: 0.3203\n",
      "Epoch 651/1000\n",
      "36324/36324 [==============================] - 28s 761us/step - loss: 0.0107 - val_loss: 0.3813\n",
      "Epoch 652/1000\n",
      "36324/36324 [==============================] - 27s 744us/step - loss: 0.0110 - val_loss: 0.3289\n",
      "Epoch 653/1000\n",
      "36324/36324 [==============================] - 24s 662us/step - loss: 0.0105 - val_loss: 0.3207\n",
      "Epoch 654/1000\n",
      "36324/36324 [==============================] - 26s 717us/step - loss: 0.0120 - val_loss: 0.2782\n",
      "Epoch 655/1000\n",
      "36324/36324 [==============================] - 26s 707us/step - loss: 0.0111 - val_loss: 0.3352\n",
      "Epoch 656/1000\n",
      "36324/36324 [==============================] - 24s 673us/step - loss: 0.0102 - val_loss: 0.3709\n",
      "Epoch 657/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36324/36324 [==============================] - 23s 632us/step - loss: 0.0106 - val_loss: 0.3614\n",
      "Epoch 658/1000\n",
      "36324/36324 [==============================] - 23s 628us/step - loss: 0.0113 - val_loss: 0.4023\n",
      "Epoch 659/1000\n",
      "36324/36324 [==============================] - 24s 666us/step - loss: 0.0109 - val_loss: 0.3064\n",
      "Epoch 660/1000\n",
      "36324/36324 [==============================] - 24s 659us/step - loss: 0.0104 - val_loss: 0.3456\n",
      "Epoch 661/1000\n",
      "36324/36324 [==============================] - 24s 655us/step - loss: 0.0104 - val_loss: 0.3511\n",
      "Epoch 662/1000\n",
      "36324/36324 [==============================] - 24s 663us/step - loss: 0.0112 - val_loss: 0.4060\n",
      "Epoch 663/1000\n",
      "36324/36324 [==============================] - 24s 667us/step - loss: 0.0107 - val_loss: 0.4008\n",
      "Epoch 664/1000\n",
      "36324/36324 [==============================] - 24s 668us/step - loss: 0.0114 - val_loss: 0.4258\n",
      "Epoch 665/1000\n",
      "36324/36324 [==============================] - 23s 633us/step - loss: 0.0107 - val_loss: 0.3460\n",
      "Epoch 666/1000\n",
      "36324/36324 [==============================] - 24s 649us/step - loss: 0.0114 - val_loss: 0.3664\n",
      "Epoch 667/1000\n",
      "36324/36324 [==============================] - 22s 612us/step - loss: 0.0113 - val_loss: 0.3976\n",
      "Epoch 668/1000\n",
      "36324/36324 [==============================] - 27s 755us/step - loss: 0.0113 - val_loss: 0.3695\n",
      "Epoch 669/1000\n",
      "36324/36324 [==============================] - 83s 2ms/step - loss: 0.0108 - val_loss: 0.3074\n",
      "Epoch 670/1000\n",
      "36324/36324 [==============================] - 39s 1ms/step - loss: 0.0109 - val_loss: 0.3548\n",
      "Epoch 671/1000\n",
      "36324/36324 [==============================] - 25s 698us/step - loss: 0.0108 - val_loss: 0.3094\n",
      "Epoch 672/1000\n",
      "36324/36324 [==============================] - 26s 728us/step - loss: 0.0104 - val_loss: 0.3573\n",
      "Epoch 673/1000\n",
      "36324/36324 [==============================] - 24s 656us/step - loss: 0.0105 - val_loss: 0.3533\n",
      "Epoch 674/1000\n",
      "36324/36324 [==============================] - 25s 687us/step - loss: 0.0108 - val_loss: 0.3346\n",
      "Epoch 675/1000\n",
      "36324/36324 [==============================] - 23s 644us/step - loss: 0.0103 - val_loss: 0.4080\n",
      "Epoch 676/1000\n",
      "36324/36324 [==============================] - 22s 619us/step - loss: 0.0105 - val_loss: 0.3534\n",
      "Epoch 677/1000\n",
      "36324/36324 [==============================] - 24s 650us/step - loss: 0.0121 - val_loss: 0.3713\n",
      "Epoch 678/1000\n",
      "36324/36324 [==============================] - 26s 709us/step - loss: 0.0113 - val_loss: 0.3454\n",
      "Epoch 679/1000\n",
      "36324/36324 [==============================] - 22s 608us/step - loss: 0.0116 - val_loss: 0.4008\n",
      "Epoch 680/1000\n",
      "36324/36324 [==============================] - 23s 647us/step - loss: 0.0111 - val_loss: 0.3848\n",
      "Epoch 681/1000\n",
      "36324/36324 [==============================] - 24s 650us/step - loss: 0.0120 - val_loss: 0.2739\n",
      "Epoch 682/1000\n",
      "36324/36324 [==============================] - 23s 641us/step - loss: 0.0115 - val_loss: 0.3693\n",
      "Epoch 683/1000\n",
      "36324/36324 [==============================] - 25s 681us/step - loss: 0.0109 - val_loss: 0.3319\n",
      "Epoch 684/1000\n",
      "36324/36324 [==============================] - 23s 646us/step - loss: 0.0105 - val_loss: 0.3610\n",
      "Epoch 685/1000\n",
      "36324/36324 [==============================] - 23s 624us/step - loss: 0.0106 - val_loss: 0.3346\n",
      "Epoch 686/1000\n",
      "36324/36324 [==============================] - 23s 630us/step - loss: 0.0111 - val_loss: 0.3713\n",
      "Epoch 687/1000\n",
      "36324/36324 [==============================] - 22s 605us/step - loss: 0.0111 - val_loss: 0.3584\n",
      "Epoch 688/1000\n",
      "36324/36324 [==============================] - 22s 610us/step - loss: 0.0114 - val_loss: 0.4216\n",
      "Epoch 689/1000\n",
      "36324/36324 [==============================] - 21s 586us/step - loss: 0.0118 - val_loss: 0.2594\n",
      "Epoch 690/1000\n",
      "36324/36324 [==============================] - 23s 624us/step - loss: 0.0109 - val_loss: 0.3922\n",
      "Epoch 691/1000\n",
      "36324/36324 [==============================] - 24s 655us/step - loss: 0.0110 - val_loss: 0.2958\n",
      "Epoch 692/1000\n",
      "36324/36324 [==============================] - 22s 599us/step - loss: 0.0111 - val_loss: 0.3689\n",
      "Epoch 693/1000\n",
      "36324/36324 [==============================] - 21s 571us/step - loss: 0.0118 - val_loss: 0.2895\n",
      "Epoch 694/1000\n",
      "36324/36324 [==============================] - 22s 606us/step - loss: 0.0127 - val_loss: 0.2679\n",
      "Epoch 695/1000\n",
      "36324/36324 [==============================] - 22s 600us/step - loss: 0.0115 - val_loss: 0.2504\n",
      "Epoch 696/1000\n",
      "36324/36324 [==============================] - 22s 603us/step - loss: 0.0121 - val_loss: 0.2906\n",
      "Epoch 697/1000\n",
      "36324/36324 [==============================] - 21s 590us/step - loss: 0.0113 - val_loss: 0.3102\n",
      "Epoch 698/1000\n",
      "36324/36324 [==============================] - 22s 609us/step - loss: 0.0114 - val_loss: 0.3599\n",
      "Epoch 699/1000\n",
      "36324/36324 [==============================] - 24s 666us/step - loss: 0.0115 - val_loss: 0.3269\n",
      "Epoch 700/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 625s 17ms/step - loss: 0.0110 - val_loss: 0.3281\n",
      "Epoch 701/1000\n",
      "36324/36324 [==============================] - 22s 607us/step - loss: 0.0130 - val_loss: 0.3353\n",
      "Epoch 702/1000\n",
      "36324/36324 [==============================] - 24s 659us/step - loss: 0.0112 - val_loss: 0.3263\n",
      "Epoch 703/1000\n",
      "36324/36324 [==============================] - 24s 661us/step - loss: 0.0114 - val_loss: 0.3238\n",
      "Epoch 704/1000\n",
      "36324/36324 [==============================] - 22s 611us/step - loss: 0.0123 - val_loss: 0.2986\n",
      "Epoch 705/1000\n",
      "36324/36324 [==============================] - 24s 662us/step - loss: 0.0124 - val_loss: 0.2998\n",
      "Epoch 706/1000\n",
      "36324/36324 [==============================] - 22s 617us/step - loss: 0.0117 - val_loss: 0.3546\n",
      "Epoch 707/1000\n",
      "36324/36324 [==============================] - 24s 648us/step - loss: 0.0128 - val_loss: 0.2791\n",
      "Epoch 708/1000\n",
      "36324/36324 [==============================] - 37s 1ms/step - loss: 0.0112 - val_loss: 0.2974\n",
      "Epoch 709/1000\n",
      "36324/36324 [==============================] - 27s 742us/step - loss: 0.0115 - val_loss: 0.3602\n",
      "Epoch 710/1000\n",
      "36324/36324 [==============================] - 24s 662us/step - loss: 0.0118 - val_loss: 0.2894\n",
      "Epoch 711/1000\n",
      "36324/36324 [==============================] - 24s 673us/step - loss: 0.0110 - val_loss: 0.3498\n",
      "Epoch 712/1000\n",
      "36324/36324 [==============================] - 23s 622us/step - loss: 0.0118 - val_loss: 0.3185\n",
      "Epoch 713/1000\n",
      "36324/36324 [==============================] - 21s 582us/step - loss: 0.0113 - val_loss: 0.4567\n",
      "Epoch 714/1000\n",
      "36324/36324 [==============================] - 22s 596us/step - loss: 0.0111 - val_loss: 0.2832\n",
      "Epoch 715/1000\n",
      "36324/36324 [==============================] - 20s 564us/step - loss: 0.0118 - val_loss: 0.3358\n",
      "Epoch 716/1000\n",
      "36324/36324 [==============================] - 20s 552us/step - loss: 0.0120 - val_loss: 0.4234\n",
      "Epoch 717/1000\n",
      "36324/36324 [==============================] - 20s 556us/step - loss: 0.0117 - val_loss: 0.3375\n",
      "Epoch 718/1000\n",
      "36324/36324 [==============================] - 21s 577us/step - loss: 0.0103 - val_loss: 0.3150\n",
      "Epoch 719/1000\n",
      "36324/36324 [==============================] - 23s 630us/step - loss: 0.0126 - val_loss: 0.4382\n",
      "Epoch 720/1000\n",
      "36324/36324 [==============================] - 25s 686us/step - loss: 0.0107 - val_loss: 0.2987\n",
      "Epoch 721/1000\n",
      "36324/36324 [==============================] - 25s 686us/step - loss: 0.0108 - val_loss: 0.3755\n",
      "Epoch 722/1000\n",
      "36324/36324 [==============================] - 25s 678us/step - loss: 0.0111 - val_loss: 0.3708\n",
      "Epoch 723/1000\n",
      "36324/36324 [==============================] - 29s 811us/step - loss: 0.0109 - val_loss: 0.2839\n",
      "Epoch 724/1000\n",
      "36324/36324 [==============================] - 22s 612us/step - loss: 0.0116 - val_loss: 0.4912\n",
      "Epoch 725/1000\n",
      "36324/36324 [==============================] - 22s 613us/step - loss: 0.0128 - val_loss: 0.4611\n",
      "Epoch 726/1000\n",
      "36324/36324 [==============================] - 24s 673us/step - loss: 0.0114 - val_loss: 0.3236\n",
      "Epoch 727/1000\n",
      "36324/36324 [==============================] - 22s 606us/step - loss: 0.0105 - val_loss: 0.3131\n",
      "Epoch 728/1000\n",
      "36324/36324 [==============================] - 21s 586us/step - loss: 0.0115 - val_loss: 0.4483\n",
      "Epoch 729/1000\n",
      "36324/36324 [==============================] - 21s 585us/step - loss: 0.0119 - val_loss: 0.3527\n",
      "Epoch 730/1000\n",
      "36324/36324 [==============================] - 22s 606us/step - loss: 0.0130 - val_loss: 0.2951\n",
      "Epoch 731/1000\n",
      "36324/36324 [==============================] - 22s 602us/step - loss: 0.0136 - val_loss: 0.3994\n",
      "Epoch 732/1000\n",
      "36324/36324 [==============================] - 21s 587us/step - loss: 0.0121 - val_loss: 0.4130\n",
      "Epoch 733/1000\n",
      "36324/36324 [==============================] - 22s 593us/step - loss: 0.0128 - val_loss: 0.2403\n",
      "Epoch 734/1000\n",
      "36324/36324 [==============================] - 21s 568us/step - loss: 0.0128 - val_loss: 0.3360\n",
      "Epoch 735/1000\n",
      "36324/36324 [==============================] - 21s 565us/step - loss: 0.0116 - val_loss: 0.5147\n",
      "Epoch 736/1000\n",
      "36324/36324 [==============================] - 21s 588us/step - loss: 0.0118 - val_loss: 0.3559\n",
      "Epoch 737/1000\n",
      "36324/36324 [==============================] - 21s 567us/step - loss: 0.0116 - val_loss: 0.5226\n",
      "Epoch 738/1000\n",
      "36324/36324 [==============================] - 21s 579us/step - loss: 0.0115 - val_loss: 0.2937\n",
      "Epoch 739/1000\n",
      "36324/36324 [==============================] - 20s 550us/step - loss: 0.0119 - val_loss: 0.3460\n",
      "Epoch 740/1000\n",
      "36324/36324 [==============================] - 20s 551us/step - loss: 0.0107 - val_loss: 0.3108\n",
      "Epoch 741/1000\n",
      "36324/36324 [==============================] - 22s 594us/step - loss: 0.0109 - val_loss: 0.5346\n",
      "Epoch 742/1000\n",
      "36324/36324 [==============================] - 23s 643us/step - loss: 0.0125 - val_loss: 0.3773\n",
      "Epoch 743/1000\n",
      "36324/36324 [==============================] - 23s 645us/step - loss: 0.0117 - val_loss: 0.8227\n",
      "Epoch 744/1000\n",
      "36324/36324 [==============================] - 23s 638us/step - loss: 0.0125 - val_loss: 0.4459\n",
      "Epoch 745/1000\n",
      "36324/36324 [==============================] - 21s 589us/step - loss: 0.0108 - val_loss: 0.3996\n",
      "Epoch 746/1000\n",
      "36324/36324 [==============================] - 23s 622us/step - loss: 0.0124 - val_loss: 0.3716\n",
      "Epoch 747/1000\n",
      "36324/36324 [==============================] - 23s 626us/step - loss: 0.0110 - val_loss: 0.3623\n",
      "Epoch 748/1000\n",
      "36324/36324 [==============================] - 21s 581us/step - loss: 0.0116 - val_loss: 0.4509\n",
      "Epoch 749/1000\n",
      "36324/36324 [==============================] - 20s 559us/step - loss: 0.0123 - val_loss: 0.3257\n",
      "Epoch 750/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 625s 17ms/step - loss: 0.0107 - val_loss: 0.3978\n",
      "Epoch 751/1000\n",
      "36324/36324 [==============================] - 24s 663us/step - loss: 0.0114 - val_loss: 0.3624\n",
      "Epoch 752/1000\n",
      "36324/36324 [==============================] - 25s 693us/step - loss: 0.0163 - val_loss: 0.2981\n",
      "Epoch 753/1000\n",
      "36324/36324 [==============================] - 24s 648us/step - loss: 0.0163 - val_loss: 0.4029\n",
      "Epoch 754/1000\n",
      "36324/36324 [==============================] - 23s 641us/step - loss: 0.0120 - val_loss: 0.4349\n",
      "Epoch 755/1000\n",
      "36324/36324 [==============================] - 23s 641us/step - loss: 0.0121 - val_loss: 0.3695\n",
      "Epoch 756/1000\n",
      "36324/36324 [==============================] - 23s 645us/step - loss: 0.0112 - val_loss: 0.3355\n",
      "Epoch 757/1000\n",
      "36324/36324 [==============================] - 22s 612us/step - loss: 0.0115 - val_loss: 0.3531\n",
      "Epoch 758/1000\n",
      "36324/36324 [==============================] - 24s 667us/step - loss: 0.0121 - val_loss: 0.3317\n",
      "Epoch 759/1000\n",
      "36324/36324 [==============================] - 22s 612us/step - loss: 0.0119 - val_loss: 0.4696\n",
      "Epoch 760/1000\n",
      "36324/36324 [==============================] - 24s 666us/step - loss: 0.0119 - val_loss: 0.4246\n",
      "Epoch 761/1000\n",
      "36324/36324 [==============================] - 24s 671us/step - loss: 0.0113 - val_loss: 0.3358\n",
      "Epoch 762/1000\n",
      "36324/36324 [==============================] - 26s 704us/step - loss: 0.0120 - val_loss: 0.3313\n",
      "Epoch 763/1000\n",
      "36324/36324 [==============================] - 25s 675us/step - loss: 0.0100 - val_loss: 0.3569\n",
      "Epoch 764/1000\n",
      "36324/36324 [==============================] - 23s 637us/step - loss: 0.0110 - val_loss: 0.3777\n",
      "Epoch 765/1000\n",
      "36324/36324 [==============================] - 22s 596us/step - loss: 0.0112 - val_loss: 0.3842\n",
      "Epoch 766/1000\n",
      "36324/36324 [==============================] - 21s 586us/step - loss: 0.0111 - val_loss: 0.3231\n",
      "Epoch 767/1000\n",
      "36324/36324 [==============================] - 22s 611us/step - loss: 0.0111 - val_loss: 0.4041\n",
      "Epoch 768/1000\n",
      "36324/36324 [==============================] - 21s 583us/step - loss: 0.0103 - val_loss: 0.3371\n",
      "Epoch 769/1000\n",
      "36324/36324 [==============================] - 22s 615us/step - loss: 0.0111 - val_loss: 0.2712\n",
      "Epoch 770/1000\n",
      "36324/36324 [==============================] - 22s 600us/step - loss: 0.0117 - val_loss: 0.4204\n",
      "Epoch 771/1000\n",
      "36324/36324 [==============================] - 25s 695us/step - loss: 0.0112 - val_loss: 0.3577\n",
      "Epoch 772/1000\n",
      "36324/36324 [==============================] - 29s 811us/step - loss: 0.0116 - val_loss: 0.5514\n",
      "Epoch 773/1000\n",
      "36324/36324 [==============================] - 27s 747us/step - loss: 0.0112 - val_loss: 0.3672\n",
      "Epoch 774/1000\n",
      "36324/36324 [==============================] - 26s 716us/step - loss: 0.0110 - val_loss: 0.4074\n",
      "Epoch 775/1000\n",
      "36324/36324 [==============================] - 25s 701us/step - loss: 0.0110 - val_loss: 0.4291\n",
      "Epoch 776/1000\n",
      "36324/36324 [==============================] - 24s 667us/step - loss: 0.0113 - val_loss: 0.3055\n",
      "Epoch 777/1000\n",
      "36324/36324 [==============================] - 23s 631us/step - loss: 0.0109 - val_loss: 0.3680\n",
      "Epoch 778/1000\n",
      "36324/36324 [==============================] - 24s 661us/step - loss: 0.0109 - val_loss: 0.3047\n",
      "Epoch 779/1000\n",
      "36324/36324 [==============================] - 22s 619us/step - loss: 0.0107 - val_loss: 0.3344\n",
      "Epoch 780/1000\n",
      "36324/36324 [==============================] - 24s 655us/step - loss: 0.0114 - val_loss: 0.2972\n",
      "Epoch 781/1000\n",
      "36324/36324 [==============================] - 22s 616us/step - loss: 0.0118 - val_loss: 0.7061\n",
      "Epoch 782/1000\n",
      "36324/36324 [==============================] - 22s 611us/step - loss: 0.0110 - val_loss: 0.4059\n",
      "Epoch 783/1000\n",
      "36324/36324 [==============================] - 21s 585us/step - loss: 0.0105 - val_loss: 0.4075\n",
      "Epoch 784/1000\n",
      "36324/36324 [==============================] - 22s 609us/step - loss: 0.0109 - val_loss: 0.3484\n",
      "Epoch 785/1000\n",
      "36324/36324 [==============================] - 28s 762us/step - loss: 0.0171 - val_loss: 0.5019\n",
      "Epoch 786/1000\n",
      "36324/36324 [==============================] - 23s 625us/step - loss: 0.0150 - val_loss: 0.2976\n",
      "Epoch 787/1000\n",
      "36324/36324 [==============================] - 27s 748us/step - loss: 0.0127 - val_loss: 0.3475\n",
      "Epoch 788/1000\n",
      "36324/36324 [==============================] - 26s 706us/step - loss: 0.0121 - val_loss: 0.3579\n",
      "Epoch 789/1000\n",
      "36324/36324 [==============================] - 26s 727us/step - loss: 0.0126 - val_loss: 0.3364\n",
      "Epoch 790/1000\n",
      "36324/36324 [==============================] - 26s 704us/step - loss: 0.0112 - val_loss: 0.2669\n",
      "Epoch 791/1000\n",
      "36324/36324 [==============================] - 23s 641us/step - loss: 0.0126 - val_loss: 0.3913\n",
      "Epoch 792/1000\n",
      "36324/36324 [==============================] - 26s 718us/step - loss: 0.0105 - val_loss: 0.3092\n",
      "Epoch 793/1000\n",
      "36324/36324 [==============================] - 26s 717us/step - loss: 0.0116 - val_loss: 0.3075\n",
      "Epoch 794/1000\n",
      "36324/36324 [==============================] - 26s 725us/step - loss: 0.0103 - val_loss: 0.2825\n",
      "Epoch 795/1000\n",
      "36324/36324 [==============================] - 27s 740us/step - loss: 0.0112 - val_loss: 0.2785\n",
      "Epoch 796/1000\n",
      "36324/36324 [==============================] - 25s 689us/step - loss: 0.0120 - val_loss: 0.2987\n",
      "Epoch 797/1000\n",
      "36324/36324 [==============================] - 25s 691us/step - loss: 0.0109 - val_loss: 0.2991\n",
      "Epoch 798/1000\n",
      "36324/36324 [==============================] - 26s 705us/step - loss: 0.0121 - val_loss: 0.4153\n",
      "Epoch 799/1000\n",
      "36324/36324 [==============================] - 25s 683us/step - loss: 0.0123 - val_loss: 0.3132\n",
      "Epoch 800/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 658s 18ms/step - loss: 0.0114 - val_loss: 0.3965\n",
      "Epoch 801/1000\n",
      "36324/36324 [==============================] - 30s 830us/step - loss: 0.0107 - val_loss: 0.3475\n",
      "Epoch 802/1000\n",
      "36324/36324 [==============================] - 32s 871us/step - loss: 0.0120 - val_loss: 0.2632\n",
      "Epoch 803/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36324/36324 [==============================] - 30s 814us/step - loss: 0.0113 - val_loss: 0.3522\n",
      "Epoch 804/1000\n",
      "36324/36324 [==============================] - 27s 736us/step - loss: 0.0112 - val_loss: 0.3186\n",
      "Epoch 805/1000\n",
      "36324/36324 [==============================] - 26s 705us/step - loss: 0.0111 - val_loss: 0.3379\n",
      "Epoch 806/1000\n",
      "36324/36324 [==============================] - 26s 715us/step - loss: 0.0112 - val_loss: 0.2815\n",
      "Epoch 807/1000\n",
      "36324/36324 [==============================] - 25s 691us/step - loss: 0.0109 - val_loss: 0.3613\n",
      "Epoch 808/1000\n",
      "36324/36324 [==============================] - 25s 684us/step - loss: 0.0103 - val_loss: 0.3169\n",
      "Epoch 809/1000\n",
      "36324/36324 [==============================] - 25s 675us/step - loss: 0.0120 - val_loss: 0.3179\n",
      "Epoch 810/1000\n",
      "36324/36324 [==============================] - 25s 683us/step - loss: 0.0114 - val_loss: 0.3043\n",
      "Epoch 811/1000\n",
      "36324/36324 [==============================] - 26s 708us/step - loss: 0.0113 - val_loss: 0.3933\n",
      "Epoch 812/1000\n",
      "36324/36324 [==============================] - 23s 643us/step - loss: 0.0107 - val_loss: 0.2756\n",
      "Epoch 813/1000\n",
      "36324/36324 [==============================] - 27s 738us/step - loss: 0.0110 - val_loss: 0.3881\n",
      "Epoch 814/1000\n",
      "36324/36324 [==============================] - 29s 786us/step - loss: 0.0113 - val_loss: 0.4299\n",
      "Epoch 815/1000\n",
      "36324/36324 [==============================] - 28s 780us/step - loss: 0.0112 - val_loss: 0.4020\n",
      "Epoch 816/1000\n",
      "36324/36324 [==============================] - 28s 776us/step - loss: 0.0106 - val_loss: 0.3286\n",
      "Epoch 817/1000\n",
      "36324/36324 [==============================] - 26s 729us/step - loss: 0.0113 - val_loss: 0.3839\n",
      "Epoch 818/1000\n",
      "36324/36324 [==============================] - 27s 743us/step - loss: 0.0100 - val_loss: 0.3767\n",
      "Epoch 819/1000\n",
      "36324/36324 [==============================] - 25s 677us/step - loss: 0.0109 - val_loss: 0.5424\n",
      "Epoch 820/1000\n",
      "36324/36324 [==============================] - 26s 703us/step - loss: 0.0104 - val_loss: 0.4074\n",
      "Epoch 821/1000\n",
      "36324/36324 [==============================] - 24s 672us/step - loss: 0.0112 - val_loss: 0.3386\n",
      "Epoch 822/1000\n",
      "36324/36324 [==============================] - 23s 621us/step - loss: 0.0107 - val_loss: 0.6194\n",
      "Epoch 823/1000\n",
      "36324/36324 [==============================] - 28s 763us/step - loss: 0.0109 - val_loss: 0.3094\n",
      "Epoch 824/1000\n",
      "36324/36324 [==============================] - 29s 801us/step - loss: 0.0110 - val_loss: 0.3582\n",
      "Epoch 825/1000\n",
      "36324/36324 [==============================] - 26s 727us/step - loss: 0.0123 - val_loss: 0.2858\n",
      "Epoch 826/1000\n",
      "36324/36324 [==============================] - 25s 682us/step - loss: 0.0125 - val_loss: 0.3397\n",
      "Epoch 827/1000\n",
      "36324/36324 [==============================] - 24s 673us/step - loss: 0.0113 - val_loss: 0.4299\n",
      "Epoch 828/1000\n",
      "36324/36324 [==============================] - 43s 1ms/step - loss: 0.0115 - val_loss: 0.3061\n",
      "Epoch 829/1000\n",
      "36324/36324 [==============================] - 28s 761us/step - loss: 0.0120 - val_loss: 0.3960\n",
      "Epoch 830/1000\n",
      "36324/36324 [==============================] - 24s 652us/step - loss: 0.0103 - val_loss: 0.3882\n",
      "Epoch 831/1000\n",
      "36324/36324 [==============================] - 22s 610us/step - loss: 0.0109 - val_loss: 0.3667\n",
      "Epoch 832/1000\n",
      "36324/36324 [==============================] - 24s 673us/step - loss: 0.0111 - val_loss: 0.3163\n",
      "Epoch 833/1000\n",
      "36324/36324 [==============================] - 23s 641us/step - loss: 0.0140 - val_loss: 0.3965\n",
      "Epoch 834/1000\n",
      "36324/36324 [==============================] - 27s 730us/step - loss: 0.0113 - val_loss: 0.3192\n",
      "Epoch 835/1000\n",
      "36324/36324 [==============================] - 25s 698us/step - loss: 0.0120 - val_loss: 0.3415\n",
      "Epoch 836/1000\n",
      "36324/36324 [==============================] - 23s 643us/step - loss: 0.0112 - val_loss: 0.4678\n",
      "Epoch 837/1000\n",
      "36324/36324 [==============================] - 22s 599us/step - loss: 0.0118 - val_loss: 0.2795\n",
      "Epoch 838/1000\n",
      "36324/36324 [==============================] - 24s 666us/step - loss: 0.0122 - val_loss: 0.3873\n",
      "Epoch 839/1000\n",
      "36324/36324 [==============================] - 23s 642us/step - loss: 0.0113 - val_loss: 0.3271\n",
      "Epoch 840/1000\n",
      "36324/36324 [==============================] - 23s 631us/step - loss: 0.0104 - val_loss: 0.3258\n",
      "Epoch 841/1000\n",
      "36324/36324 [==============================] - 24s 660us/step - loss: 0.0103 - val_loss: 0.3417\n",
      "Epoch 842/1000\n",
      "36324/36324 [==============================] - 24s 672us/step - loss: 0.0184 - val_loss: 0.2902\n",
      "Epoch 843/1000\n",
      "36324/36324 [==============================] - 25s 689us/step - loss: 0.0151 - val_loss: 0.4427\n",
      "Epoch 844/1000\n",
      "36324/36324 [==============================] - 25s 675us/step - loss: 0.0133 - val_loss: 0.2833\n",
      "Epoch 845/1000\n",
      "36324/36324 [==============================] - 26s 702us/step - loss: 0.0141 - val_loss: 0.3137\n",
      "Epoch 846/1000\n",
      "36324/36324 [==============================] - 23s 642us/step - loss: 0.0129 - val_loss: 0.2973\n",
      "Epoch 847/1000\n",
      "36324/36324 [==============================] - 37s 1ms/step - loss: 0.0111 - val_loss: 0.3178\n",
      "Epoch 848/1000\n",
      "36324/36324 [==============================] - 23s 635us/step - loss: 0.0125 - val_loss: 0.3114\n",
      "Epoch 849/1000\n",
      "36324/36324 [==============================] - 23s 629us/step - loss: 0.0109 - val_loss: 0.2737\n",
      "Epoch 850/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 669s 18ms/step - loss: 0.0120 - val_loss: 0.2384\n",
      "Epoch 851/1000\n",
      "36324/36324 [==============================] - 89s 2ms/step - loss: 0.0106 - val_loss: 0.3458\n",
      "Epoch 852/1000\n",
      "36324/36324 [==============================] - 32s 868us/step - loss: 0.0127 - val_loss: 0.2896\n",
      "Epoch 853/1000\n",
      "36324/36324 [==============================] - 27s 736us/step - loss: 0.0110 - val_loss: 0.3061\n",
      "Epoch 854/1000\n",
      "36324/36324 [==============================] - 25s 677us/step - loss: 0.0111 - val_loss: 0.3028\n",
      "Epoch 855/1000\n",
      "36324/36324 [==============================] - 24s 670us/step - loss: 0.0109 - val_loss: 0.2603\n",
      "Epoch 856/1000\n",
      "36324/36324 [==============================] - 25s 687us/step - loss: 0.0125 - val_loss: 0.3346\n",
      "Epoch 857/1000\n",
      "36324/36324 [==============================] - 28s 767us/step - loss: 0.0117 - val_loss: 0.2942\n",
      "Epoch 858/1000\n",
      "36324/36324 [==============================] - 24s 665us/step - loss: 0.0110 - val_loss: 0.3590\n",
      "Epoch 859/1000\n",
      "36324/36324 [==============================] - 24s 666us/step - loss: 0.0106 - val_loss: 0.3039\n",
      "Epoch 860/1000\n",
      "36324/36324 [==============================] - 24s 674us/step - loss: 0.0104 - val_loss: 0.3328\n",
      "Epoch 861/1000\n",
      "36324/36324 [==============================] - 28s 783us/step - loss: 0.0123 - val_loss: 0.3511\n",
      "Epoch 862/1000\n",
      "36324/36324 [==============================] - 28s 766us/step - loss: 0.0108 - val_loss: 0.2895\n",
      "Epoch 863/1000\n",
      "36324/36324 [==============================] - 25s 679us/step - loss: 0.0107 - val_loss: 0.3882\n",
      "Epoch 864/1000\n",
      "36324/36324 [==============================] - 25s 692us/step - loss: 0.0112 - val_loss: 0.3433\n",
      "Epoch 865/1000\n",
      "36324/36324 [==============================] - 25s 687us/step - loss: 0.0111 - val_loss: 0.3228\n",
      "Epoch 866/1000\n",
      "36324/36324 [==============================] - 23s 643us/step - loss: 0.0110 - val_loss: 0.3623\n",
      "Epoch 867/1000\n",
      "36324/36324 [==============================] - 25s 675us/step - loss: 0.0099 - val_loss: 0.4245\n",
      "Epoch 868/1000\n",
      "36324/36324 [==============================] - 23s 632us/step - loss: 0.0103 - val_loss: 0.2840\n",
      "Epoch 869/1000\n",
      "36324/36324 [==============================] - 23s 633us/step - loss: 0.0108 - val_loss: 0.2744\n",
      "Epoch 870/1000\n",
      "36324/36324 [==============================] - 24s 652us/step - loss: 0.0109 - val_loss: 0.3921\n",
      "Epoch 871/1000\n",
      "36324/36324 [==============================] - 24s 647us/step - loss: 0.0117 - val_loss: 0.2740\n",
      "Epoch 872/1000\n",
      "36324/36324 [==============================] - 23s 645us/step - loss: 0.0111 - val_loss: 0.3909\n",
      "Epoch 873/1000\n",
      "36324/36324 [==============================] - 23s 635us/step - loss: 0.0101 - val_loss: 0.3247\n",
      "Epoch 874/1000\n",
      "36324/36324 [==============================] - 23s 646us/step - loss: 0.0113 - val_loss: 0.2659\n",
      "Epoch 875/1000\n",
      "36324/36324 [==============================] - 24s 664us/step - loss: 0.0117 - val_loss: 0.3213\n",
      "Epoch 876/1000\n",
      "36324/36324 [==============================] - 25s 702us/step - loss: 0.0101 - val_loss: 0.2722\n",
      "Epoch 877/1000\n",
      "36324/36324 [==============================] - 24s 662us/step - loss: 0.0110 - val_loss: 0.2671\n",
      "Epoch 878/1000\n",
      "36324/36324 [==============================] - 24s 665us/step - loss: 0.0109 - val_loss: 0.3245\n",
      "Epoch 879/1000\n",
      "36324/36324 [==============================] - 22s 618us/step - loss: 0.0108 - val_loss: 0.3753\n",
      "Epoch 880/1000\n",
      "36324/36324 [==============================] - 23s 644us/step - loss: 0.0104 - val_loss: 0.3260\n",
      "Epoch 881/1000\n",
      "36324/36324 [==============================] - 25s 695us/step - loss: 0.0113 - val_loss: 0.3461\n",
      "Epoch 882/1000\n",
      "36324/36324 [==============================] - 24s 655us/step - loss: 0.0106 - val_loss: 0.3235\n",
      "Epoch 883/1000\n",
      "36324/36324 [==============================] - 24s 650us/step - loss: 0.0122 - val_loss: 0.3260\n",
      "Epoch 884/1000\n",
      "36324/36324 [==============================] - 25s 679us/step - loss: 0.0115 - val_loss: 0.4521\n",
      "Epoch 885/1000\n",
      "36324/36324 [==============================] - 23s 642us/step - loss: 0.0104 - val_loss: 0.2887\n",
      "Epoch 886/1000\n",
      "36324/36324 [==============================] - 25s 686us/step - loss: 0.0104 - val_loss: 0.2916\n",
      "Epoch 887/1000\n",
      "36324/36324 [==============================] - 27s 746us/step - loss: 0.0113 - val_loss: 0.3173\n",
      "Epoch 888/1000\n",
      "36324/36324 [==============================] - 23s 644us/step - loss: 0.0110 - val_loss: 0.3037\n",
      "Epoch 889/1000\n",
      "36324/36324 [==============================] - 26s 705us/step - loss: 0.0116 - val_loss: 0.3193\n",
      "Epoch 890/1000\n",
      "36324/36324 [==============================] - 24s 650us/step - loss: 0.0106 - val_loss: 0.3745\n",
      "Epoch 891/1000\n",
      "36324/36324 [==============================] - 23s 634us/step - loss: 0.0108 - val_loss: 0.3486\n",
      "Epoch 892/1000\n",
      "36324/36324 [==============================] - 23s 639us/step - loss: 0.0104 - val_loss: 0.3413\n",
      "Epoch 893/1000\n",
      "36324/36324 [==============================] - 22s 614us/step - loss: 0.0109 - val_loss: 0.3445\n",
      "Epoch 894/1000\n",
      "36324/36324 [==============================] - 26s 719us/step - loss: 0.0116 - val_loss: 0.3440\n",
      "Epoch 895/1000\n",
      "36324/36324 [==============================] - 22s 608us/step - loss: 0.0111 - val_loss: 0.4032\n",
      "Epoch 896/1000\n",
      "36324/36324 [==============================] - 24s 649us/step - loss: 0.0179 - val_loss: 0.3801\n",
      "Epoch 897/1000\n",
      "36324/36324 [==============================] - 24s 650us/step - loss: 0.0123 - val_loss: 0.3692\n",
      "Epoch 898/1000\n",
      "36324/36324 [==============================] - 25s 698us/step - loss: 0.0129 - val_loss: 0.3538\n",
      "Epoch 899/1000\n",
      "36324/36324 [==============================] - 26s 715us/step - loss: 0.0148 - val_loss: 0.3445\n",
      "Epoch 900/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 634s 17ms/step - loss: 0.0119 - val_loss: 0.2833\n",
      "Epoch 901/1000\n",
      "36324/36324 [==============================] - 27s 750us/step - loss: 0.0120 - val_loss: 0.3304\n",
      "Epoch 902/1000\n",
      "36324/36324 [==============================] - 25s 700us/step - loss: 0.0110 - val_loss: 0.2693\n",
      "Epoch 903/1000\n",
      "36324/36324 [==============================] - 39s 1ms/step - loss: 0.0110 - val_loss: 0.3434\n",
      "Epoch 904/1000\n",
      "36324/36324 [==============================] - 28s 784us/step - loss: 0.0109 - val_loss: 0.3202\n",
      "Epoch 905/1000\n",
      "36324/36324 [==============================] - 26s 725us/step - loss: 0.0111 - val_loss: 0.3496\n",
      "Epoch 906/1000\n",
      "36324/36324 [==============================] - 23s 629us/step - loss: 0.0111 - val_loss: 0.3298\n",
      "Epoch 907/1000\n",
      "36324/36324 [==============================] - 26s 711us/step - loss: 0.0108 - val_loss: 0.3835\n",
      "Epoch 908/1000\n",
      "36324/36324 [==============================] - 25s 676us/step - loss: 0.0105 - val_loss: 0.3539\n",
      "Epoch 909/1000\n",
      "36324/36324 [==============================] - 28s 767us/step - loss: 0.0107 - val_loss: 0.3717\n",
      "Epoch 910/1000\n",
      "36324/36324 [==============================] - 26s 707us/step - loss: 0.0110 - val_loss: 0.3046\n",
      "Epoch 911/1000\n",
      "36324/36324 [==============================] - 25s 700us/step - loss: 0.0114 - val_loss: 0.3122\n",
      "Epoch 912/1000\n",
      "36324/36324 [==============================] - 29s 792us/step - loss: 0.0102 - val_loss: 0.3625\n",
      "Epoch 913/1000\n",
      "36324/36324 [==============================] - 25s 690us/step - loss: 0.0106 - val_loss: 0.2969\n",
      "Epoch 914/1000\n",
      "36324/36324 [==============================] - 26s 713us/step - loss: 0.0099 - val_loss: 0.2720\n",
      "Epoch 915/1000\n",
      "36324/36324 [==============================] - 24s 672us/step - loss: 0.0110 - val_loss: 0.3565\n",
      "Epoch 916/1000\n",
      "36324/36324 [==============================] - 24s 650us/step - loss: 0.0104 - val_loss: 0.2968\n",
      "Epoch 917/1000\n",
      "36324/36324 [==============================] - 27s 734us/step - loss: 0.0106 - val_loss: 0.3917\n",
      "Epoch 918/1000\n",
      "36324/36324 [==============================] - 24s 648us/step - loss: 0.0110 - val_loss: 0.3494\n",
      "Epoch 919/1000\n",
      "36324/36324 [==============================] - 24s 661us/step - loss: 0.0111 - val_loss: 0.4016\n",
      "Epoch 920/1000\n",
      "36324/36324 [==============================] - 23s 630us/step - loss: 0.0107 - val_loss: 0.3748\n",
      "Epoch 921/1000\n",
      "36324/36324 [==============================] - 26s 723us/step - loss: 0.0107 - val_loss: 0.2898\n",
      "Epoch 922/1000\n",
      "36324/36324 [==============================] - 26s 716us/step - loss: 0.0111 - val_loss: 0.4445\n",
      "Epoch 923/1000\n",
      "36324/36324 [==============================] - 26s 706us/step - loss: 0.0102 - val_loss: 0.3246\n",
      "Epoch 924/1000\n",
      "36324/36324 [==============================] - 25s 684us/step - loss: 0.0112 - val_loss: 0.4000\n",
      "Epoch 925/1000\n",
      "36324/36324 [==============================] - 24s 657us/step - loss: 0.0103 - val_loss: 0.3654\n",
      "Epoch 926/1000\n",
      "36324/36324 [==============================] - 23s 645us/step - loss: 0.0102 - val_loss: 0.3213\n",
      "Epoch 927/1000\n",
      "36324/36324 [==============================] - 23s 641us/step - loss: 0.0102 - val_loss: 0.3122\n",
      "Epoch 928/1000\n",
      "36324/36324 [==============================] - 25s 694us/step - loss: 0.0110 - val_loss: 0.3174\n",
      "Epoch 929/1000\n",
      "36324/36324 [==============================] - 29s 798us/step - loss: 0.0108 - val_loss: 0.3037\n",
      "Epoch 930/1000\n",
      "36324/36324 [==============================] - 24s 660us/step - loss: 0.0106 - val_loss: 0.2943\n",
      "Epoch 931/1000\n",
      "36324/36324 [==============================] - 26s 723us/step - loss: 0.0109 - val_loss: 0.3020\n",
      "Epoch 932/1000\n",
      "36324/36324 [==============================] - 24s 652us/step - loss: 0.0120 - val_loss: 0.2618\n",
      "Epoch 933/1000\n",
      "36324/36324 [==============================] - 26s 725us/step - loss: 0.0116 - val_loss: 0.3350\n",
      "Epoch 934/1000\n",
      "36324/36324 [==============================] - 24s 663us/step - loss: 0.0102 - val_loss: 0.3515\n",
      "Epoch 935/1000\n",
      "36324/36324 [==============================] - 25s 675us/step - loss: 0.0116 - val_loss: 0.2610\n",
      "Epoch 936/1000\n",
      "36324/36324 [==============================] - 24s 674us/step - loss: 0.0189 - val_loss: 0.4905\n",
      "Epoch 937/1000\n",
      "36324/36324 [==============================] - 24s 655us/step - loss: 0.0182 - val_loss: 0.3085\n",
      "Epoch 938/1000\n",
      "36324/36324 [==============================] - 26s 717us/step - loss: 0.0145 - val_loss: 0.3201\n",
      "Epoch 939/1000\n",
      "36324/36324 [==============================] - 25s 692us/step - loss: 0.0125 - val_loss: 0.3331\n",
      "Epoch 940/1000\n",
      "36324/36324 [==============================] - 24s 671us/step - loss: 0.0114 - val_loss: 0.3398\n",
      "Epoch 941/1000\n",
      "36324/36324 [==============================] - 23s 639us/step - loss: 0.0120 - val_loss: 0.3331\n",
      "Epoch 942/1000\n",
      "36324/36324 [==============================] - 25s 689us/step - loss: 0.0112 - val_loss: 0.2932\n",
      "Epoch 943/1000\n",
      "36324/36324 [==============================] - 23s 624us/step - loss: 0.0109 - val_loss: 0.3375\n",
      "Epoch 944/1000\n",
      "36324/36324 [==============================] - 23s 638us/step - loss: 0.0113 - val_loss: 0.3578\n",
      "Epoch 945/1000\n",
      "36324/36324 [==============================] - 26s 702us/step - loss: 0.0112 - val_loss: 0.3441\n",
      "Epoch 946/1000\n",
      "36324/36324 [==============================] - 25s 685us/step - loss: 0.0113 - val_loss: 0.3321\n",
      "Epoch 947/1000\n",
      "36324/36324 [==============================] - 24s 669us/step - loss: 0.0109 - val_loss: 0.2969\n",
      "Epoch 948/1000\n",
      "36324/36324 [==============================] - 24s 668us/step - loss: 0.0115 - val_loss: 0.3119\n",
      "Epoch 949/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36324/36324 [==============================] - 26s 711us/step - loss: 0.0108 - val_loss: 0.3089\n",
      "Epoch 950/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 647s 18ms/step - loss: 0.0113 - val_loss: 0.3330\n",
      "Epoch 951/1000\n",
      "36324/36324 [==============================] - 25s 699us/step - loss: 0.0116 - val_loss: 0.3248\n",
      "Epoch 952/1000\n",
      "36324/36324 [==============================] - 27s 748us/step - loss: 0.0108 - val_loss: 0.2943\n",
      "Epoch 953/1000\n",
      "36324/36324 [==============================] - 24s 655us/step - loss: 0.0103 - val_loss: 0.3818\n",
      "Epoch 954/1000\n",
      "36324/36324 [==============================] - 23s 645us/step - loss: 0.0111 - val_loss: 0.3728\n",
      "Epoch 955/1000\n",
      "36324/36324 [==============================] - 23s 627us/step - loss: 0.0115 - val_loss: 0.3022\n",
      "Epoch 956/1000\n",
      "36324/36324 [==============================] - 25s 690us/step - loss: 0.0110 - val_loss: 0.3048\n",
      "Epoch 957/1000\n",
      "36324/36324 [==============================] - 23s 646us/step - loss: 0.0107 - val_loss: 0.3020\n",
      "Epoch 958/1000\n",
      "36324/36324 [==============================] - 26s 709us/step - loss: 0.0105 - val_loss: 0.3223\n",
      "Epoch 959/1000\n",
      "36324/36324 [==============================] - 23s 632us/step - loss: 0.0104 - val_loss: 0.2991\n",
      "Epoch 960/1000\n",
      "36324/36324 [==============================] - 23s 645us/step - loss: 0.0115 - val_loss: 0.4010\n",
      "Epoch 961/1000\n",
      "36324/36324 [==============================] - 23s 619us/step - loss: 0.0098 - val_loss: 0.3155\n",
      "Epoch 962/1000\n",
      "36324/36324 [==============================] - 23s 624us/step - loss: 0.0104 - val_loss: 0.2968\n",
      "Epoch 963/1000\n",
      "36324/36324 [==============================] - 24s 668us/step - loss: 0.0104 - val_loss: 0.3275\n",
      "Epoch 964/1000\n",
      "36324/36324 [==============================] - 24s 653us/step - loss: 0.0096 - val_loss: 0.3189\n",
      "Epoch 965/1000\n",
      "36324/36324 [==============================] - 25s 698us/step - loss: 0.0101 - val_loss: 0.2845\n",
      "Epoch 966/1000\n",
      "36324/36324 [==============================] - 24s 656us/step - loss: 0.0134 - val_loss: 0.3132\n",
      "Epoch 967/1000\n",
      "36324/36324 [==============================] - 25s 678us/step - loss: 0.0125 - val_loss: 0.2563\n",
      "Epoch 968/1000\n",
      "36324/36324 [==============================] - 24s 663us/step - loss: 0.0122 - val_loss: 0.3191\n",
      "Epoch 969/1000\n",
      "36324/36324 [==============================] - 24s 648us/step - loss: 0.0117 - val_loss: 0.3335\n",
      "Epoch 970/1000\n",
      "36324/36324 [==============================] - 23s 635us/step - loss: 0.0127 - val_loss: 0.2693\n",
      "Epoch 971/1000\n",
      "36324/36324 [==============================] - 24s 648us/step - loss: 0.0137 - val_loss: 0.2986\n",
      "Epoch 972/1000\n",
      "36324/36324 [==============================] - 24s 648us/step - loss: 0.0119 - val_loss: 0.2348\n",
      "Epoch 973/1000\n",
      "36324/36324 [==============================] - 24s 653us/step - loss: 0.0111 - val_loss: 0.3470\n",
      "Epoch 974/1000\n",
      "36324/36324 [==============================] - 22s 615us/step - loss: 0.0111 - val_loss: 0.2624\n",
      "Epoch 975/1000\n",
      "36324/36324 [==============================] - 24s 647us/step - loss: 0.0114 - val_loss: 0.3036\n",
      "Epoch 976/1000\n",
      "36324/36324 [==============================] - 25s 683us/step - loss: 0.0109 - val_loss: 0.2948\n",
      "Epoch 977/1000\n",
      "36324/36324 [==============================] - 23s 623us/step - loss: 0.0118 - val_loss: 0.4231\n",
      "Epoch 978/1000\n",
      "36324/36324 [==============================] - 26s 710us/step - loss: 0.0102 - val_loss: 0.2531\n",
      "Epoch 979/1000\n",
      "36324/36324 [==============================] - 22s 610us/step - loss: 0.0117 - val_loss: 0.2159\n",
      "Epoch 980/1000\n",
      "36324/36324 [==============================] - 22s 617us/step - loss: 0.0117 - val_loss: 0.2488\n",
      "Epoch 981/1000\n",
      "36324/36324 [==============================] - 23s 624us/step - loss: 0.0099 - val_loss: 0.3703\n",
      "Epoch 982/1000\n",
      "36324/36324 [==============================] - 24s 652us/step - loss: 0.0118 - val_loss: 0.2701\n",
      "Epoch 983/1000\n",
      "36324/36324 [==============================] - 23s 631us/step - loss: 0.0105 - val_loss: 0.3164\n",
      "Epoch 984/1000\n",
      "36324/36324 [==============================] - 24s 660us/step - loss: 0.0105 - val_loss: 0.3075\n",
      "Epoch 985/1000\n",
      "36324/36324 [==============================] - 24s 661us/step - loss: 0.0112 - val_loss: 0.3272\n",
      "Epoch 986/1000\n",
      "36324/36324 [==============================] - 23s 636us/step - loss: 0.0104 - val_loss: 0.3275\n",
      "Epoch 987/1000\n",
      "36324/36324 [==============================] - 24s 670us/step - loss: 0.0108 - val_loss: 0.3212\n",
      "Epoch 988/1000\n",
      "36324/36324 [==============================] - 22s 609us/step - loss: 0.0101 - val_loss: 0.3722\n",
      "Epoch 989/1000\n",
      "36324/36324 [==============================] - 24s 668us/step - loss: 0.0115 - val_loss: 0.2966\n",
      "Epoch 990/1000\n",
      "36324/36324 [==============================] - 25s 677us/step - loss: 0.0102 - val_loss: 0.3496\n",
      "Epoch 991/1000\n",
      "36324/36324 [==============================] - 23s 636us/step - loss: 0.0111 - val_loss: 0.3360\n",
      "Epoch 992/1000\n",
      "36324/36324 [==============================] - 24s 663us/step - loss: 0.0104 - val_loss: 0.2736\n",
      "Epoch 993/1000\n",
      "36324/36324 [==============================] - 23s 641us/step - loss: 0.0107 - val_loss: 0.3206\n",
      "Epoch 994/1000\n",
      "36324/36324 [==============================] - 24s 674us/step - loss: 0.0122 - val_loss: 0.2187\n",
      "Epoch 995/1000\n",
      "36324/36324 [==============================] - 25s 683us/step - loss: 0.0111 - val_loss: 0.2430\n",
      "Epoch 996/1000\n",
      "36324/36324 [==============================] - 24s 670us/step - loss: 0.0106 - val_loss: 0.3564\n",
      "Epoch 997/1000\n",
      "36324/36324 [==============================] - 24s 652us/step - loss: 0.0108 - val_loss: 0.2549\n",
      "Epoch 998/1000\n",
      "36324/36324 [==============================] - 22s 612us/step - loss: 0.0102 - val_loss: 0.2939\n",
      "Epoch 999/1000\n",
      "36324/36324 [==============================] - 24s 649us/step - loss: 0.0099 - val_loss: 0.2532\n",
      "Epoch 1000/1000\n",
      "cooling down to decrease CPU tempereture\n",
      "36324/36324 [==============================] - 701s 19ms/step - loss: 0.0106 - val_loss: 0.4976\n",
      "time took 10:25:12.478818\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start=dt.datetime.now()\n",
    "results=model.fit(train_im,train_ang,epochs=1000,batch_size=512,validation_data=(val_im,val_ang),verbose=1,callbacks=[delay()])\n",
    "print(\"time took\",dt.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Best_model_Two/mymodel_best_model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(results.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_csv_file = 'Best_model_Two/history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_json_file = 'Best_model_Two/history.json' \n",
    "with open(hist_json_file, mode='w') as f:\n",
    "    df.to_json(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_excel = df.to_excel (r'Best_model_Two/export_dataframe.xlsx', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 4s 424us/step\n",
      "0.497568832510395\n",
      "36324/36324 [==============================] - 42s 1ms/step\n",
      "loss: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0023767097456279785"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model=load_model(\"Best_model_Two/mymodel_best_model.ckpt\")\n",
    "scores = model.evaluate(val_im, val_ang, verbose=1)\n",
    "print(scores)\n",
    "scores = model.evaluate(train_im, train_ang, verbose=1)\n",
    "print(\"loss: \" % (scores))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.271318</td>\n",
       "      <td>0.292208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.181091</td>\n",
       "      <td>0.233669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.170120</td>\n",
       "      <td>0.173623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.237940</td>\n",
       "      <td>0.150316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.215207</td>\n",
       "      <td>0.103878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.235417</td>\n",
       "      <td>0.083101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.235007</td>\n",
       "      <td>0.069757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.192635</td>\n",
       "      <td>0.069413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.287003</td>\n",
       "      <td>0.062364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.187805</td>\n",
       "      <td>0.053428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.188568</td>\n",
       "      <td>0.046367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.205654</td>\n",
       "      <td>0.046014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.229547</td>\n",
       "      <td>0.045242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.216781</td>\n",
       "      <td>0.041020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.214109</td>\n",
       "      <td>0.041292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.208807</td>\n",
       "      <td>0.041699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.226790</td>\n",
       "      <td>0.037723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.237139</td>\n",
       "      <td>0.037719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.206844</td>\n",
       "      <td>0.033876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.211476</td>\n",
       "      <td>0.033370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.199536</td>\n",
       "      <td>0.034970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.217469</td>\n",
       "      <td>0.032050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.222514</td>\n",
       "      <td>0.035249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.249400</td>\n",
       "      <td>0.033281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.215408</td>\n",
       "      <td>0.032036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.231404</td>\n",
       "      <td>0.030941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.234023</td>\n",
       "      <td>0.033263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.261584</td>\n",
       "      <td>0.030573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.247139</td>\n",
       "      <td>0.031862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.218925</td>\n",
       "      <td>0.029861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>0.298571</td>\n",
       "      <td>0.013705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>0.234785</td>\n",
       "      <td>0.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>0.347029</td>\n",
       "      <td>0.011107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>0.262365</td>\n",
       "      <td>0.011061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>0.303572</td>\n",
       "      <td>0.011379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>0.294786</td>\n",
       "      <td>0.010887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>0.423106</td>\n",
       "      <td>0.011774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>0.253065</td>\n",
       "      <td>0.010179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>0.215883</td>\n",
       "      <td>0.011698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>0.248827</td>\n",
       "      <td>0.011652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0.370340</td>\n",
       "      <td>0.009891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>0.270134</td>\n",
       "      <td>0.011768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>0.316421</td>\n",
       "      <td>0.010550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>0.307508</td>\n",
       "      <td>0.010471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>0.327174</td>\n",
       "      <td>0.011197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>0.327486</td>\n",
       "      <td>0.010436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>0.321185</td>\n",
       "      <td>0.010794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>0.372181</td>\n",
       "      <td>0.010119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>0.296569</td>\n",
       "      <td>0.011506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>0.349628</td>\n",
       "      <td>0.010202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>0.336042</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>0.273607</td>\n",
       "      <td>0.010435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>0.320558</td>\n",
       "      <td>0.010659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>0.218693</td>\n",
       "      <td>0.012198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>0.242991</td>\n",
       "      <td>0.011090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.356352</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.254857</td>\n",
       "      <td>0.010789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.293893</td>\n",
       "      <td>0.010208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.253182</td>\n",
       "      <td>0.009884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.497569</td>\n",
       "      <td>0.010581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     val_loss      loss\n",
       "0    0.271318  0.292208\n",
       "1    0.181091  0.233669\n",
       "2    0.170120  0.173623\n",
       "3    0.237940  0.150316\n",
       "4    0.215207  0.103878\n",
       "5    0.235417  0.083101\n",
       "6    0.235007  0.069757\n",
       "7    0.192635  0.069413\n",
       "8    0.287003  0.062364\n",
       "9    0.187805  0.053428\n",
       "10   0.188568  0.046367\n",
       "11   0.205654  0.046014\n",
       "12   0.229547  0.045242\n",
       "13   0.216781  0.041020\n",
       "14   0.214109  0.041292\n",
       "15   0.208807  0.041699\n",
       "16   0.226790  0.037723\n",
       "17   0.237139  0.037719\n",
       "18   0.206844  0.033876\n",
       "19   0.211476  0.033370\n",
       "20   0.199536  0.034970\n",
       "21   0.217469  0.032050\n",
       "22   0.222514  0.035249\n",
       "23   0.249400  0.033281\n",
       "24   0.215408  0.032036\n",
       "25   0.231404  0.030941\n",
       "26   0.234023  0.033263\n",
       "27   0.261584  0.030573\n",
       "28   0.247139  0.031862\n",
       "29   0.218925  0.029861\n",
       "..        ...       ...\n",
       "970  0.298571  0.013705\n",
       "971  0.234785  0.011900\n",
       "972  0.347029  0.011107\n",
       "973  0.262365  0.011061\n",
       "974  0.303572  0.011379\n",
       "975  0.294786  0.010887\n",
       "976  0.423106  0.011774\n",
       "977  0.253065  0.010179\n",
       "978  0.215883  0.011698\n",
       "979  0.248827  0.011652\n",
       "980  0.370340  0.009891\n",
       "981  0.270134  0.011768\n",
       "982  0.316421  0.010550\n",
       "983  0.307508  0.010471\n",
       "984  0.327174  0.011197\n",
       "985  0.327486  0.010436\n",
       "986  0.321185  0.010794\n",
       "987  0.372181  0.010119\n",
       "988  0.296569  0.011506\n",
       "989  0.349628  0.010202\n",
       "990  0.336042  0.011111\n",
       "991  0.273607  0.010435\n",
       "992  0.320558  0.010659\n",
       "993  0.218693  0.012198\n",
       "994  0.242991  0.011090\n",
       "995  0.356352  0.010593\n",
       "996  0.254857  0.010789\n",
       "997  0.293893  0.010208\n",
       "998  0.253182  0.009884\n",
       "999  0.497569  0.010581\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "963"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmin(df[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss    963\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['loss']].idxmin() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>0.318928</td>\n",
       "      <td>0.009587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     val_loss      loss\n",
       "963  0.318928  0.009587"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['loss']==df['loss'][963]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"Best_model_Two/history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.271318</td>\n",
       "      <td>0.292208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.181091</td>\n",
       "      <td>0.233669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.170120</td>\n",
       "      <td>0.173623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.237940</td>\n",
       "      <td>0.150316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.215207</td>\n",
       "      <td>0.103878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  val_loss      loss\n",
       "0           0  0.271318  0.292208\n",
       "1           1  0.181091  0.233669\n",
       "2           2  0.170120  0.173623\n",
       "3           3  0.237940  0.150316\n",
       "4           4  0.215207  0.103878"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      "Unnamed: 0    1000 non-null int64\n",
      "val_loss      1000 non-null float64\n",
      "loss          1000 non-null float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 23.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01481919353181154"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"loss\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>499.500000</td>\n",
       "      <td>0.316213</td>\n",
       "      <td>0.014819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>288.819436</td>\n",
       "      <td>0.060234</td>\n",
       "      <td>0.014598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170120</td>\n",
       "      <td>0.009587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>249.750000</td>\n",
       "      <td>0.277187</td>\n",
       "      <td>0.011270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>499.500000</td>\n",
       "      <td>0.311064</td>\n",
       "      <td>0.012215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>749.250000</td>\n",
       "      <td>0.346074</td>\n",
       "      <td>0.014137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>999.000000</td>\n",
       "      <td>0.822659</td>\n",
       "      <td>0.292208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0     val_loss         loss\n",
       "count  1000.000000  1000.000000  1000.000000\n",
       "mean    499.500000     0.316213     0.014819\n",
       "std     288.819436     0.060234     0.014598\n",
       "min       0.000000     0.170120     0.009587\n",
       "25%     249.750000     0.277187     0.011270\n",
       "50%     499.500000     0.311064     0.012215\n",
       "75%     749.250000     0.346074     0.014137\n",
       "max     999.000000     0.822659     0.292208"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuu",
   "language": "python",
   "name": "gpuu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
